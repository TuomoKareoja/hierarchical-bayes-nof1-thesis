\documentclass[12pt,a4paper,leqno]{report}

\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{etoolbox}
\usepackage{pythontex}
\usepackage{booktabs}
\usepackage{bookmark}
\usepackage{float}

\usetikzlibrary{matrix}

\newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}} \newcommand{\N}{\mathbb{N}}
\newcommand{\No}{\mathbb{N}_0} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\mathleft}{\@fleqntrue\@mathmargin0pt}

\theoremstyle{plain}
\newtheorem{equa}[equation]{Equation}
\newtheorem{lem}[equation]{Lemma}
\newtheorem{prop}[equation]{Proposition}
\newtheorem{cor}[equation]{Corollary}

\theoremstyle{definition}
\newtheorem{defi}[equation]{definition}
\newtheorem{conj}[equation]{Conjecture}
\newtheorem{example}[equation]{Example}

\theoremstyle{remark}
\newtheorem{note}[equation]{Note}

\pagestyle{plain}
\setcounter{page}{1}
\addtolength{\hoffset}{-1.15cm}
\addtolength{\textwidth}{2.3cm}
\addtolength{\voffset}{0.45cm}
\addtolength{\textheight}{-0.9cm}

\graphicspath{{../figures/}}
\epstopdfsetup{outdir=../figures/}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\title{Applying Bayesian Hierarchical Models to N-of-1 Trials}
\author{Tuomo Kareoja}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\chapter*{Introduction}\label{intro}
\addcontentsline{toc}{chapter}{Introduction}

In clinical practice, N-of-1 trials are multiple crossover trials conducted on a
single patient, where treatment periods are formed into multiple blocks each
containing at least one period of each treatment under
consideration\ \cite{nofone}. By comparing the measurements taken during different
treatment regimes over multiple blocks, the most suitable treatment option can
be chosen for the particular patient studied.

In the following pages, we will walk trough a concise explanation of the
experimental design of N-of-1 trials and how we can statistically model them,
given the kinds of challenges their design poses. We then follow up with how to
apply Bayesian inference with these models to estimate the effectiveness of different
treatments, highlighting how Bayesian methods can give needed flexibility when
running N-of-1 trials by not relying on hypothesis testing tied to a
prespecified study design. We then consider the case when there are multiple
similar N-of-1 trials and show how it is possible to pool the information from
these with hierarchical Bayesian methods, without loosing sight of the goal of
N-of-1 trials: to find the best treatment for each individual
patient. Finally, we end by with a complete example of analyzing multiple N-of-1
trials with hierarchical Bayesian methods with Python and PyMC3-package using
simulated data.

\chapter{What Are N-of-1 Trials?}\label{nof1}

In the assessment of any medical treatment the ``gold standard'' is a randomized
controlled trial (RCT), where subjects are randomized to two or more groups that
are given different treatments or no treatment at all. The measurements from
these groups are then compared and a result derived about which treatment is
most effective on average. This design takes into account unknown factors that
might make some patients more suspectable to certain treatments by forming the
groups randomly and thus, on average, distributing these patients evenly between
groups. By comparing the groups against each other and not just to the same
patients at the beginning and end of the study, it also takes into account time-related
effects like the natural progression of a disease. Despite its unquestionable value
in finding general effects, this method can run into problems when we
try to apply its results to individual patients in clinical practice.

Knowing the best treatment on average might not help much in finding the right
treatment for a particular patient if the variability in treatment effectiveness
between patients is high. Although we can use covariates like age, gender, or
a certain gene variant to explain the variance between patients in RCT:s, there
can still be lots of unexplained between-patient variance left. Some
of this variance is of course caused by random factors like measurement error,
but a significant part might be caused by real differences between the patients\ \cite{HTE1, HTE2, HTE3, HTE4}.
In other words, there might be individual factors that explain the variability
of the efficacy of different treatments between patients that might either be
too specific to be considered in an RCT study or unknown and thus impossible to
analyze in this kind of experimental design.

Another problem with RCTs is the peculiarity of their participants. It is
a common practice to accept patients to RCTs only if they don't suffer from any
medical issue besides the one that is being studied. This lack of comorbidity
makes it easier to get clear results by removing confounding factors, but at the
same time this lowers the external validity of the results, because in the real
world patients often suffer from multiple medical issues simultaneously. Because of this, it might be possible that even when the scientific
literature provides clear results about the relative effectiveness of different
treatments, these results don't generalize that well to the kinds of
patients the clinical practitioner sees daily in her office.\ \cite{HTE1}

N-of-1 trials can be used to patch the holes in the knowledge that RCTs cannot
fill by changing the focus of the study from group averages to the individual
patients. In N-of-1 studies, instead of comparing groups with different treatments,
the comparisons are made across time with a single patient using different
treatments. In this design multiple treatments are tried sequentially in periods
that are formed into blocks each of which contains a treatment period of each
treatment option under consideration at least once. The treatment periods themselves are
arranged in a random or balanced order to take
into account time-related effects. A simple example would be an ABBA design that
includes two blocks within which the two treatments A and B are assigned in a
balanced order. Measurements are taken throughout the study and, depending on the
ease of measuring the outcome of interest, this could mean anything
once per treatment period to continuous monitoring (e.g.\ heart rate).

Depending on the treatments considered, there might also be a so-called
``washout'' period between treatments, where the patient does not receive any
treatment, and her state is allowed to return to baseline. Washout periods are used to
prevent treatment interactions that could make it difficult to analyze the
results or could be dangerous to the patient. If the treatments being studied
allow it, N-of-1 trial can also use the double-blind method, where both the patient
and the person administering the treatment don't know which treatment is currently used, and placebo
treatment, where one of the treatments only resembles treatment, but does not
contain any active ingredient (pharmacological or otherwise).

In figure\ \ref{examplenof1schema} is a schema of a more complex N-of-1 trial with three treatments A, B and C,
random assignment of treatments within blocks, three measurements within each
treatment period and washout periods between treatments:

\begin{figure}[H]
    \centering
    \caption{Example of a N-of-1 Experiment Design}\label{examplenof1schema}
    \includegraphics{n-of-1_schema.pdf}
\end{figure}

The stated aim of N-of-1 trials is quite different from RCTs: where the latter
tries to generalize results to the population and find which treatment is best in
general, the N-of-1 trial tries to only generalize to the patient in question.
This means that whereas comorbidity and other factors that cause systematic
variation between the patients in treatment outcomes are a problem in RCTs,
these are not an issue in N-of-1 trials because there is no need to generalize
beyond the one patient. There is also often no actual need to know what is
causing a certain treatment to work better or worse, as long as it is not
because of measurement errors or time-related effects.

Use of N-of-1 trials is appropriate in situations where there are multiple
treatment options but no prior knowledge of which of these would be
best, when there is known to be considerable variability between patients in
treatment efficacy, or when there is reason to doubt that the results from
scientific literature generalize to the patient in question\ \cite{nofone}. This
would seem to make n-of-1 trials applicable to many situations, but there are
also multiple factors restricting their use.

Firstly, N-of-1 trials can only be used to study illnesses that are chronic,
progress slowly, and are at least somewhat stable. Also the treatment options
available must have noticeable treatment responses within a short timeframe.
Running the trials needs time to complete and fast-changing illness or slowly
effecting treatment make it either impossible to distinguish true effects from
the natural progression of the disease or make the length of the trials
impractically long. N-of-1 trials are also unsuitable for testing of
preventative treatments because the effects of these treatments are often
impossible to assess without comparisons to other patients who are not receiving
the treatment.\ \cite{nofone}

Secondly, running N-of-1 trials is costly because of added expenses of training
the medical staff in the method, running the trial with all its measurements and
analyzing the data. This is means that it can be hard to find cases where using
this method is cost-effective and studies assessing the cost and benefits of these trials
have given mixed results\ \cite{costs}.

These limitations have kept the use of N-of-1 trials rare in clinical use, even
though they could potentially both increase the life quality of the patients and
lower the healthcare costs by finding the most suitable medications to patients that
might end up potentially using them for years. This state of affairs might be
changing fast though. Aging and environmental factors are changing the worlds'
disease burden so that a growing proportion of it is constituted by chronic
diseases\ \cite{diseaseburden}, of which common ones like non-acute cardiovascular
diseases and diabetes are excellent candidates for N-of-1 trials. Also the cost
of administering N-of-1 trials are dropping with the advent of cheap and reliable
health sensors like smartwatches and connected blood pressure monitors. For
example in diabetic patients it is now possible to get real-time readings of
blood insulin levels with minimal effort from the patient\ \cite{cgm}. These
ongoing changes mean that the popularity of this method could potentially rise
significantly in the future.

\chapter{Statistical Modeling of N-of-1 Trials}\label{modeling}

Even though the data created by the N-of-1 trials resembles traditional time
series data with autocorrelation between observations and repeated measurements
from the same study unit, there are additional complexities rising from the structure of
repeating treatment periods. Trying to take into account all
the peculiarities of the study design could end up with a model that is too complicated to
the small amount of data generated by a single study, so one must consider
carefully what factors actually need to be incorporated into the model.

Simplest model that we could employ is to just count the number of blocks where
a treatment is considered ``better'' than others. The precise definition of
``better'' doesn't matter here. This way we arrive to a simple binomial model
where the number of ``successes'' \(X\) is the number of blocks where a
treatment is considered the ``best'' follows binomial distribution and the
probability of each treatment option of having \(k\) successes is given by:

\begin{def}\label{}
    \begin{equation}
        P(X = k) = {n\choose k}p^k{(1-p)}^{(n-k)},
    \end{equation}
\end{def}where \(k\) = number of blocks, where the treatments is considered the
``best'', \(n\) = total number of blocks and \(p\) = the probability of being
considered the ``best''.

This type of model is rudimentary at best because it fails to consider the
magnitude of the differences between treatment effects and does not take into
account the actual number of measurements within each treatment period. To take
these factors into account, more complex models are in order.

\section{Basic Models}\label{conti}

Before going further we make the assumption that the measurements we are dealing with are continuous
as this is probably the most common case. Lets first look at a model where
we assume that there are no time-trends and no autocorrelation between
measurements. Let \(y_{mbpt}\) represent the outcome measured while on treatment
\(m\) within treatment block \(b\) within treatment period \(p\) at time \(t\).
The treatment periods are indexed within each block and time is indexed within
each treatment period:

\begin{def}\label{}
    \begin{equation}\label{allerrors}
        y_{mbpt} = \mu_m + \gamma_b + \delta_{p(b)} + \epsilon_{t(p(b))},
    \end{equation}
\end{def}where \(\gamma_b \sim N(0,\sigma^2_{\gamma})\), \(\delta_{p(b)} \sim
N(0,\sigma^2_{\delta})\), and \(\epsilon_{t(p(b))} \sim
N(0,\sigma^2_{\epsilon})\)

This model assumes all treatment effects \(\mu_m\) to be constant. Within the
normally distributed terms \(\gamma_b\) represents random block effects,
\(\delta_{p(b)}\) random treatment period effects and \(\epsilon_{t(p(b))}\) random within
period errors. We could also choose one of the blocks as a reference and set
\(\gamma_1 = 0\) and assume that within each block the between period effects
follow the same pattern, e.g.\ the difference between treatment period one and
two is the same within each block. The random effects between blocks and treatment
periods could represent for example the random variations in the motivation of the patient and possible
changes in treating personnel within each block and treatment period. The random
within period errors represents the measurement error of single measurements
within treatment periods. The relative size of these terms is important
for effective design of the trial, because they determine if it is more
beneficial for the statistical power of the study to add more measurements,
treatment periods or blocks.

If the measurements within blocks and treatment periods do not correlate, the
model\ \ref{allerrors} can be greatly simplified by dropping \(\gamma_b\) and
\(\delta_{p(b)}\):

\begin{def}\label{}
    \begin{equation}\label{oneerror}
        y_{mbpt} = \mu_m + \epsilon_{t(p(b))},
    \end{equation}
\end{def}where \(\epsilon_{t(p(b))} \sim N(0,\sigma^2_{\epsilon})\).

This simple model could be considered a starting point in modelling a successful trial where
the treatments and measurements were administered consistently, there were no big changes to the staff interacting
with patient and no modifications to any other procedures were made during
the trial.

\section{Incorporating time-trends into the model}\label{timetrends}

As the symptoms of the patient might not be completely stable (e.g.\ because
symptoms get worse with the progression of the disease) adding some kind of
time-trend to the model is usually advisable. We can modify the simple model\ \ref{oneerror} from previous chapter to include a linear time-trend by adding an
intercept and slope of the time-trend. In this case the model can expressed more
concisely just in terms of the measurement \(y_t\) taken at time \(t\), where
time is indexed from the start of the study:

\begin{def}\label{}
    \begin{equation}\label{simpletimetrend}
        y_t = \beta_0 + \beta_1 t + \mu_m + \epsilon_t,
    \end{equation}
\end{def}where \(\epsilon_t \sim N(0,\sigma^2)\).

Here \(\beta_0\) is the intercept, \(\beta_1\) the slope of the time trend,
\(\mu_m\) the effect of the treatment given during time \(t\) and \(\epsilon_t\)
is the residual error at time \(t\). More complex time-trends can be introduced
by modifying the slope, for example by adding the term \(\beta_2 t^2\) to
introduce a quadratic trend.

Another time dependent effect to take into consideration is period effects.
There might be some part of the trial that falls within a period that we presume
to have its own effect. An example of this kind of effect is if we study asthma
medications and part of the trial falls within the pollen season. A simple way
to model this is to use a dummy variable that takes the value 1 within the
period and 0 outside it. Extending the model\ \ref{simpletimetrend} with a period of constant
effect \(\beta_2\) we end up with:

\begin{def}\label{}
    \begin{equation}\label{}
        y_t = \beta_0 + \beta_1 t + \beta_{2}Z_t + \mu_t + \epsilon_t,
    \end{equation}
\end{def}where \(\epsilon_t \sim N(0,\sigma^2)\) and dummy variable \(Z_t = 1\)
when \(t \in (t_{period\,start} \ldots t_{period\,end}) \) and \(0\) otherwise.

Lastly, to take into account that treatment effects themselves can vary with
time, for example because treatment works better during periods of greater
disease severity, we can add a time-by-treatment interaction effect into the
model. For example in the case where we expect that the illness gets steadily
(linearly) worse with time, but the treatments compensate this by being
similarly more effective, we can extend the simple linear
time-trend model\ \ref{simpletimetrend} by adding a second time related
treatment term \(\gamma_m t\) which describes the effect of treatment \(m \) given at
time \(t \) in regard to time:

\begin{def}\label{}
    \begin{equation}\label{}
        y_t = \beta_0 + \beta_1 t + \mu_m + \gamma_m t + \epsilon_t,
    \end{equation}
\end{def}

In general, systematic and complex time-related effects are difficult to
deal with in N-of-1 studies as the amount of data created is typically small
and complex parametrisations can quickly lead to models that too complex to efficiently
estimate. Some of this problem can avoided by good experiment design (for example not
conducting trials where we expect any period effects), but unfortunately this does not
help with illnesses that progress in complicated fashion and with the time-dependent
treatment effects related to this progress.

\section{The Problem of Autocorrelation}\label{autocor}

A common occurrence in time-series data is the autocorrelation between
measurements so that there is some similarity between observations defined by a
function of the time lag between them. Often a good first response to this problem
is to add a time-trend to the model like we did previously. This
often removes a substantial proportion of the autocorrelation that could be
caused for example by the natural progression of the disease\ \cite{stat}.
Unfortunately, in N-of-1 trials the carryover effects
from previous treatments and slow onset of treatment effects can lead to very
complex autocorrelation patterns that are hard to remove with just a simple time-trend.

Carryover effects refer to the lingering effects of the treatment even after it
has been stopped. This can make the treatment effects in the next treatment period
with a different treatment seem larger than they actually are or smaller in the
unfortunate, and hopefully rare, case where the previous treatment was actually
harmful. Carryover effects also encompass the effects of interactions between
sequential treatments, which could even be dangerous depending on the nature of
the treatments. On the other hand treatment effects that manifest slowly can
often give the opposite effect of carryover effects by making the treatments
look less effective than they really are during the first measurements of each
treatment period.\cite{stat}

To deal with these more devious sources of autocorrelation in our model we can
take two routes: moving average models and autoregressive models. In moving
average models the error is represented as depending on the previous error(s), by
expressing the measurement at time \(t\) as function of one or more previous
errors:

\begin{def}\label{}
    \begin{equation}\label{movingaverage}
        y_t = \mu_m + \epsilon_t + \rho\epsilon_{(t - 1)},
    \end{equation}
\end{def}where \(\mu_m\) is the effect of the treatment given at time t, \(\rho
\) is the correlation between consecutive errors and all errors \(\epsilon_t \sim
N(0,\sigma^2) \). Instead of making the measurement dependent on just the previous error the model
can be adjusted to include more a complex lag by adding more error terms with
bigger lag and including more \(\rho \) terms to denote the correlations between
them (\(\rho_1\epsilon_{t-1} + \ldots + \rho_x\epsilon_{t-x}\)).

In autoregressive models approach we express the autocorrelation in the
measurements themselves so that the measurement at time \(t\) is a function of
the measurement at \(t-1\):

\begin{def}\label{}
    \begin{equation}\label{}
        y_t = \rho y_{t-1} + \mu_m + \epsilon_t,
    \end{equation}
\end{def}where \(\mu_m\) is the effect of the treatment given at time \(t\),
\(\rho \) is the correlation between consecutive measurements and \(\epsilon_t
\sim N(0,\sigma^2)\) is the error term. Like with moving average model we could
also include more than just one lag by adding more lagged terms and correlation
terms. Although it can make more intuitive sense to make the whole observation
dependent on the previous observation, it is important to recognize that in this
case the treatment effects \(\mu_m\) must be interpreted differently as they are
in this case conditioned on previous measurements.

Moving average models and autoregressive models are often combined by using both
methods of taking the autocorrelation into account simultaneously (ARMA-models), but as
amount of data produced in N-of-1 studies if often so small, taking this approach might not be advisable as it might make the model unnecessarily
complex.

Although we can try to solve problems created by the carryover effect and the slow
manifestation of treatment effect with modeling, a better way could be to take
measures to mitigate the effects in the study design itself. By having a long
enough washout period between different treatments, we can minimize the
carryover effect. If there are no harmful interactions between the treatment the
next treatment can be started within the washout period so that we minimize the
problem of slow treatment effects. If there are interactions to be taken into
consideration, the first few measurements at the beginning of each treatment
period could also just be dropped. By doing this we are of course throwing away
data, but we must be remember that if the measurements at the beginning of the
treatment period are mangled, this will also mangle our parameter estimates.
Trying to take these effects into account in our model will probably not
eliminate these effects completely and will increase the complexity of our
model.

\section{Non-continuous Measurements}\label{noncontinuous}

Up to this point we have assumed that the measurement used are continuous, but
we could of course have measurements that are binary, counts of events or
categorical. With these kinds of measurement the models need to reformatted so that
they don't presume normal distributions. Despite this, the models don't
have to differ much from the models presented above and the principals covered
before can be applied.

To modify previously presented models to work when measurements are not
continuous, we need to formulate them as generalized linear models. We do
this by keeping the right-hand sides in the same form but expressing the
left-hand side in terms of a link function of the mean of the probability
distribution of the outcomes. So instead of expressing the model in terms of
individual observations \(y_t\), we express it as the expected value of these
measurements conditional on the data that we feed to the link
function \(E(Y|D)\). The link function allows us to model measurements with arbitrary
distributions, as now the link function can linearly depend on the parameters of
the model, rather than needing the measurements themselves to do so. We need to
do this to prevent the models from giving impossible predictions, i.e.\ negative
counts or probabilities.

Lets start with an binary outcome measurements. In this case the
measurements \(y\) at time \(t\) follow the Bernoulli distribution \(y_t \sim
Bernoulli(p)\) where the the expected value of the distribution is the
probability \(p\) of observing the event in measurement \(y_t\). In this case
the suitable link function is the logit function
\(logit(p)=\log_e(\frac{p}{1-p})\). With this information we can now formulate a
model with simple a linear time trend:

\begin{def}\label{}
    \begin{equation}
        \log_e(\frac{p_t}{1-p_t})=\beta_0 + \beta_1 t + \mu_m,
    \end{equation}
\end{def}where \(p_t\) is the the probability of observing the event at time
\(t\) and \(\log_e(\frac{p_t}{1-p_t})\) are the log-odds of this event,
\(\beta_0\) is the intercept, \(\beta_1\) the slope of the time trend and
\(\mu_m\) the effect of the treatment given during time \(t\). By first
exponentiating and then using simple algebraic manipulation we can
express the model in terms of the probability \(p_t\):

\begin{def}\label{}
    \begin{equation}\label{}
        \frac{p_t}{1-p_t}=e^{\beta_0 + \beta_1 t + \mu_m}
    \end{equation}
\end{def}

\begin{def}\label{}
    \begin{equation}\label{}
        p_t=\frac{e^{\beta_0 + \beta_1 t + \mu_m}}{e^{\beta_0 + \beta_1 t + \mu_m}+1}=\frac{1}{1+e^{-(\beta_0 + \beta_1 t + \mu_m)}}
    \end{equation}
\end{def}

We notice that there are no error terms in this model. This is because we are
not modeling individual observations, but the expected value of these values, which in
this case is the probability of observing the event under treatment used at time \(t\).
Even though there are random variations in the individual observations,
when we talk about their expected value, there is just a single value with no
random errors. Apart from this difference, we can see that we find the same model
from the denominator that we used when modeling a continuous measurement with a
linear time-trend in model\ \ref{simpletimetrend}, but without the
aforementioned error term. This means that to build the models described before,
we would just insert the right side of the equations into the denominator, apart
from the between measurements error term.

With numbers of events as measurements, instead of expressing the model in terms
of time, we can express it in terms of periods \(p\) that are of equal length
and indexed from the beginning of the experiment (this should correspond to the
treatment periods as they should be of equal length). The number of events during
period \(y_p\) follows a Poisson distribution \(y_p \sim Poisson(\lambda)\) and
the expected value of the distribution is the rate \(\lambda_p \) of the events
during period \(p\). For link function we use the natural log so that we have
\(\log_e(\lambda_p)\) on the left side of the equation. Once again using the
same simple model with linear time trend we end up with a model:

\begin{def}\label{oneerror}
    \begin{equation}
        \log_e(\lambda_p)=\beta_0 + \beta_1 r + \mu_{p},
    \end{equation}
\end{def}where \(\lambda_p\) is the the rate of events between measurements
during period \(p\), \(\beta_0\) is the intercept, \(\beta_1\) the slope of the
time trend and \(\mu_p\) the effect of the treatment given during period \(p\).
By simply exponentiating both sides we can express the model in terms of rate of
events:

\begin{def}\label{oneerror}
    \begin{equation}
        \lambda_t=e^{\beta_0 + \beta_1 t + \mu_t},
    \end{equation}
\end{def}where we now have the familiar linear model formula in the exponent on
the right side of the equation.

The case of categorical measurements is more complex as there are multiple
possible link functions depending on which way we want to model the
measurements, so we don't go trough all of them here. The one that is probably
the most relevant is the case when categorical measurements are ordinal, i.e.
they have a natural ordering like in a Likert-scale. In this case the we can
use the cumulative logit as the link function. So if we assume that our ordinal
measurement has \(J\) categorical choices ordered from 1 to \(J\), we can model
the cumulative probability of getting a measurement \(y\) at time \(t\) that is at
``smaller or equal'' to \(y\) by putting this to the logit function and using the same
basic model as before:

\begin{def}\label{oneerror}
    \begin{equation}
        \log_e\bigg({\frac{P(y_t \leq j)}{1 - P(y_t \leq j)}}\bigg)=\beta_0 + \beta_1 t + \mu_t,
    \end{equation}
\end{def}where \(\beta_0\) is the intercept, \(\beta_1\) the slope of the time
trend and \(\mu_t\) the effect of the treatment given during time \(t\). By
exponentiating and algebraic manipulation we end up with similar model as with
binary outcomes where we once again find the familiar linear model formula in
the exponent:

\begin{def}\label{}
    \begin{equation}\label{ord_exponentiating}
        \frac{P(y_t \leq j)}{1 - P(y_i \leq j)}=e^{\beta_0 + \beta_1 t + \mu_t}
    \end{equation}
\end{def}

\begin{def}\label{}
    \begin{equation}\label{ord_algmanipulation}
        P(y_t \leq j)=\frac{e^{\beta_0 + \beta_1 t + \mu_t}}{e^{\beta_0 + \beta_1 t + \mu_t}+1}=\frac{1}{1+e^{-(\beta_0 + \beta_1 t + \mu_t)}}
    \end{equation}
\end{def}

The general pattern here is that, even in the case when the measurement are not
continuous, all the modelling principals we introduced earlier still apply. The way we
think about how different parameters affect our measurements does not have to change,
even though the models need to be expressed differently.

\chapter{Bayesian Estimation}\label{bayes}

Now that we have some models defined, we need to move into the next part of the
analysis and give estimates to the parameters in these models. There
are two broad ways to approach this task by using two different definitions of
probability. The first is frequentist inference, where we consider the ``true''
parameters of our model fixed, but unknown, and randomness only applies to the
process of creating our data. The second way is Bayesian inference, where we
consider the data as fixed and instead of thinking about the parameters as
fixed parts of nature, we conceptualize them as probability distributions that
express our internal uncertainty about their true values.

Although both of these inference methods work for all the models covered before,
there are several factors that favor the use of Bayesian inference in N-of-1
studies related to their design and use context. But before talking about these factors,
we need to understand how Bayesian inference works.

\section{Principles of Bayesian Inference}\label{whybayes}

Bayesian inference is based on the Bayes' Theorem that states the probability of
an event conditional on another event:

\begin{def}\label{}
    \begin{equation}\label{bayesrule}
        P(A|B) = \frac{P(B|A)P(A)}{P(B)},
    \end{equation}
\end{def}where \(A\) and \(B\) are events and \(P(B)\) \(\neq \) 0. \(P(A|B)\) is the conditional probability of event \(A\) happening given that
event \(B\) happened and is called the posterior. \(P(A)\) is our initial
probability estimate for the event \(A\) called the prior. The quotient
\(\frac{P(B|A)}{P(B)}\) represent how much information event \(B\) gives about
event \(A\) happening. If this number is greater than 1, then event \(B\)
happening makes event \(A\) more likely and if it less than 1 is is less likely.
If the quotient is 1, event \(B\) gives no information about the probability of
event \(A\). Breaking the quotient down further \(P(B|A)\), called the likelihood, is the reverse of the posterior and tells us how believable it is to
see the event \(B\) given that event \(A\) happened. Finally \(P(B)\) in the denominator is called marginal likelihood and tells us the probability of
observing event \(B\) with or without event \(A\).

Instead of events, in our case we want to formulate the theorem with parameters
\(\Theta \) and the data \(D\), so that we get can estimate the posterior
probability (or likelihood in case of continuous parameter values) of our
parameters having certain values:

\begin{def}\label{}
    \begin{equation}\label{bayeswithparams}
        P(\Theta|D) = \frac{P(D|\Theta)P(\Theta)}{P(D)},
    \end{equation}
\end{def}where \(P(D)\) \(\neq \) 0. We could make it more explicit what the marginal likelihood \(P(D)\) stands for
by writing it as \(\sum_\Theta P(D|\Theta)P(\Theta)\) in the case when the parameter
value takes discrete values and \(\int_\Theta P(D|\Theta)P(\Theta) d\Theta \),
if they are continuous. That is, the possibility of observing the data with all
different combinations of the possible values of the parameters, taking into
account our prior beliefs in the probability of these value combinations.

To tie this formula into the models we introduced previously we can demonstrate
how this applies to a simple model with only the treatment effects \(\mu_m \) of
the given treatment and a random error \(\epsilon \) at time \(t\):

\begin{def}\label{}
    \begin{equation}\label{}
        y_t = \mu_t + \epsilon_t,
    \end{equation}
\end{def}where \(\epsilon_{t} \sim N(0,\sigma^2)\). In this case when we want to estimate the parameters of the effects of the
treatments \(\mu_m\). For simplicity lets assume that only one treatment was
given so \(\mu_t\) is constant. Let's refer to this treatment effect simply as
\(\mu \). According to our model single observations follow a normal
distribution defined by constant \(\mu \) and the error term \(\epsilon_t \). As
our model assumes that the observations \(y_t\) are independent (no
autocorrelation included in the model) individual observations are defined as
\(y_t \sim N(\mu,\sigma^2) \) and the likelihood term in the bayesian formula is
a simple product of the probability density functions of a normal distribution
with mean \(\mu \) and variance \(\sigma^2 \) that the observations are
independent:

\begin{def}\label{}
    \begin{equation}\label{}
        P(Y|\mu, \sigma)
        =
        \prod_{Y}
        \frac{1}
        {{\sqrt {2\pi \sigma^2} }}
        \exp{ \left \{ \frac{-(y_t-\mu)^2} {2\sigma^2} \right \} },
    \end{equation}
\end{def}where \(Y\) represents all the observations.

To avoid the complexity of dealing with multiple unknown variables we assume
that \(\sigma \) is known. We will later come back to the case of multiple
unknown variables. Now we have to define only one prior distribution as there is
just one unknown parameter. We choose a normal distribution for the prior of
\(\mu \) that is defined by mean \(\lambda_0 \) and variance \(\delta_0^2 \),
which are values we define ourself based on our previous knowledge and beliefs:

\begin{def}\label{}
    \begin{equation}\label{}
        P(\mu)
        =
        \frac{1}
        {{\sqrt {2\pi \delta_0^2} }}
        \exp{ \left \{\frac{-(\mu-\lambda_0)^2} {2\delta_0^2} \right \} }
    \end{equation}
\end{def}

We could of course choose whatever distribution best matches our previous
knowledge and uncertainty about the parameter, but by using a normal distribution
for prior when the likelihood also follows a normal distribution, we get some
nice algebraic properties that we will soon see.

Now the only missing term from our bayesian equation is the marginal likelihood
\(P(Y) \). When we express the term more precisely as the probability of getting
the data we observed over possible values of our unknown parameter \(\int
P(Y|\mu,\sigma^2)P(\mu) d\mu \), we can see that as we integrate over all the possible
values of \(\mu \) and \(\sigma^2\) is known, this term is constant. This means that the marginal likelihood does
not affect the shape of the posterior distribution, So the shape of posterior
distribution is defined by just by the likelihood and the prior \(P(\mu|Y)
\propto P(Y|\mu, \sigma^2)P(\mu) \). With some arithmetic manipulation we can
see that the posterior probability is actually a normal distribution with mean
\(\lambda_1 = \delta_1^2 \left( \lambda_0 \delta_0^{-2} + \sum_{t=1}^{n} y_t \sigma^{-2}
\right) \) and standard deviation \(\delta_1^2 =
\frac{1}{\delta_0^{-2}+n\delta^{-2}} \):

\begin{align}\label{normalposterior}
    P(\mu|Y) & \propto P(Y|\mu, \sigma)P(\mu)                                                                 \\
             & =
    \prod_{t=1}^{n}
    \frac{1}
    {{\sqrt {2\pi \sigma^2} }}
    \exp{ \left \{ \frac{-(y_t - \mu)^2} {2\sigma^2} \right \} }
    \frac{1}
    {{\sqrt {2\pi \delta_0^2} }}
    \exp{ \left \{ \frac{ -(\mu - \lambda_0)^2 } {2\delta_0^2} \right \} } \nonumber                          \\
             & =
    \frac{1}
    {
        (2\pi)^{\frac{n+1}{2}}
        \sqrt{ \delta_0^2 \sigma^{2n}}
    }
    \exp{
        \left \{
        \frac{- \mu^2 + 2 \mu \lambda_0 - \lambda_0^2}{2\delta_0^2}
        -
        \sum_{t=1}^{n}
        \frac{y_t^2 - 2 \mu y_t + \mu^2}{2\sigma^2}
        \right \}
    } \nonumber                                                                                               \\
             & \text{Dropping the constant terms} \nonumber                                                   \\
             & \propto
    \exp{
        \left \{
        \frac{
            -\mu^2 (\sigma^2 + t\delta_0^2)
            + 2 \mu (\lambda_0 \sigma^2 + \delta_0^2 y_1 + \cdots + \delta_0^2 y_n)
            - (\lambda_0^2 \sigma^2 + \delta_0^2 y_1^2 + \cdots + \delta_0^2 y_n^2)
        }
        {2\delta_0^2 \sigma^2}
        \right \}
    } \nonumber                                                                                               \\
             & \propto
    \exp{
        \left \{
        \frac{
            - \mu^2
            + 2 \mu \frac{\mu_0 \sigma^2 + \sum_{t=1}^{n} \delta_0^2 y_t}
            {\sigma^2 + n \delta_0^2}
            - \left(
            \frac{
                \lambda_0 \sigma^2 + \sum_{t=1}^{n} \delta_0^2 y_t
            }
            {\sigma^2 + n \delta_0^2}
            \right)^2
        }
        {
            2 \frac{\delta_0^2 \sigma^2}{\sigma^2 + n\delta_0^2}
        }
        \right \}
    }
    \times
    \exp{
        \left \{
        -\frac{
            \lambda_0 \sigma^2 + \sum_{t=1}^{n} \delta_0^2 y_t
        }
        {
            2 \delta_0^2 \sigma^2
        }
        \right \}
    } \nonumber                                                                                               \\
             & \propto
    \exp{
        \left \{
        -
        \frac{
            \left(
            \mu
            -
            \frac{
                \lambda_0 \sigma^2 + \sum_{t=1}^{n} \delta_0^2 y_t
            }
            {
                \sigma^2 + n \delta_0^2
            }
            \right)^2
        }
        {
            2 \frac{
                \delta_0^2 \sigma^2
            }
            {\sigma^2 + n \delta_0^2}
        }
        \right \}
    } \nonumber                                                                                               \\
             & =
    \exp{
        \left \{
        -
        \frac{
            \left(
            \mu
            -
            \frac{
                \lambda_0 \delta_0^{-2} + \sum_{t=1}^{n} y_t \sigma^2
            }
            {
                \delta_0^{-2} + n \sigma_0^{-2}
            }
            \right)^2
        }
        {
            2 \frac{1}
            {
                \sigma^{-2} + n \delta_0^{-2}
            }
        }
        \right \}
    } \nonumber                                                                                               \\
             & \text{Recognizing that we have the ''right'' part of normal distribution and adding} \nonumber \\
             & \text{the constant part back in} \nonumber                                                     \\
             & \propto
    \frac{1}
    {{\sqrt {2\pi \sigma^2} }}
    \exp{
        \left \{
        -
        \frac{
            \left( \mu - \lambda_1 \right)^2
        }
        {
            2 \delta_1
        }
        \right \}
    }, \text{ where } \nonumber                                                                               \\
             & \delta_1^2
    =
    \frac{1}{\delta_0^{-2}+n\delta^{-2}}
    \quad\text{and}\quad
    \lambda_1
    =
    \frac{
        \lambda_0 \delta_0^{-2} + \sum_{t=1}^{n} y_t \sigma^2
    }
    {
        \delta_0^{-2} + n \sigma_0^{-2}
    }
    =
    \delta_1^2
    \left(
    \lambda_0 \delta_0^{-2} + \sum_{t=1}^{n} y_t \sigma^{-2}
    \right) \nonumber
\end{align}

So we ended up with a posterior distribution that has the shape of a normal
distribution. The trick where we ignored the marginal likelihood can be used
anywhere, but the fact that we could arithmetically solve that the posterior
follows a well-defined distribution is not a general occurrence. Usually we need
to go beyond arithmetic methods and solve the problem algorithmically.

\section{Challenges of Bayesian Inference}\label{bayesinferencechallenges}

Although the Bayes formula is conceptually simple, but when actually utilizing
it we run into two big difficulties. First is the common case when the
probability distributions of the prior and/or the likelihood or their product
are not well-defined distributions with known properties. This can make the
integral in the marginal likelihood impossible to solve analytically like we did
above. With discrete distributions we can circumvent the problem by just counting the
likelihood times prior for every value combination and then obtaining the posterior
probabilities by dividing each of these values with their combined sum. Unfortunately
this becomes impractical very quickly when the number of parameters combinations grows
and we are left with the same challenge that we should somehow be able to solve the
marginal likelihood. The second problem is how we should define our priors.
If we have prior knowledge, we need to be able to
formulate our beliefs in a precise mathematic form and even when we don't have prior knowledge we still
need to come up with a prior, but in this case it has to be defined so that it has
minimal effects on the posterior distribution.

\subsection{Dealing with the Marginal Likelihood}\label{marginallikehoodproblems}

As mentioned above, the marginal likelihood can often be impossible or
unfeasible to calculate analytically even in the case parameters with discrete
values where we don't need to solve a complex integral. A common solution to this,
that has been historically the only option, is to rely on so-called ``conjugate
priors``. These are priors that follow distribution which multiplied by
suitable likelihood function produce a posterior distribution that has the same
form as the prior distribution. A good example of this is our example above (\ref{normalposterior}) using a normal
distribution for the prior when the likelihood is also normally distributed. If
the likelihood function belongs to the exponential family of distributions, there
exists at least one corresponding conjugate prior distribution (often also in
the exponential family). For likelihood functions outside the exponential family
there are some cases where a conjugate prior exists, but these are rare.

The conjugate function method makes the denominator analytically solvable, but it
is in practice very limiting. First, we need to restrict ourselves when
modeling to only use models that create a suitable likelihood function with an existing
conjugate prior. Second, it limits our options in defining our prior
beliefs as we have to be able to express them in the form of the conjugate function. To
be able conduct Bayesian inference in cases where can't conform to these demands we need
to turn from analytic to algorithmic solutions.

The basis for all algorithmic solutions to the problem is the fact
as the marginal likelihood is calculated over all possible parameter values, it is
not dependent on particular values of the parameter and is thus constant across
them. Therefore we can already know that the posterior will follow a distribution which
shape is defined by the likelihood times prior:

\begin{def}\label{bayespropto}
    \begin{equation}
        P(\Theta|D) \propto P(D|\Theta)P(\Theta)
    \end{equation}
\end{def}

Now if we could take a representative sample of values of this distribution we could
obtain and estimate of the posterior distribution by dividing the sampled values by
their sum. We can think of this as estimating what is the probability of getting a 6 on
suspicious die, by throwing it multiple times and counting which percentage of the
throws we get a 6. Doing this kind sampling would be easy if the distribution would have reconcilable shape like
normal distribution, but in this case we could already calculate an analytical
solution. The real problem is, how can we get a representative sample from an arbitrary
distribution where can't analytically formulate the probability density function.
The general outline for the group of algorithms trying to solve this problem called
Monte Carlo Markov Chain algorithms (MCMC) goes as follows:

\begin{enumerate}
    \item Pick a set of values for our parameters as our starting position in
          the parameter space. The starting location has to be a plausible set
          of values. As we can know that
          \(P(D|\Theta)P(\Theta)\) has the same shape as our posterior, if
          the value of this function is zero when we put our set of
          parameter values in it, these values are impossible and should not be
          uses as a starting point.
    \item Pick a second set of values for our parameters with some a random method
    \item Calculate the probability of moving from our current position in the
          parameter space to the second set of parameter values with the
          following formula:

          \begin{def}\label{randomwalk}
              \begin{equation}
                  p_{move\ to\ new\ location} = \min \bigg(\frac{P(D|\Theta = proposed\ set)P(\Theta = proposed\ set)}{P(D|\Theta = current\ set)P(\Theta = current\ set)}, 1 \bigg)
              \end{equation}
          \end{def}
          In other words, if the value calculated with the proposed parameter
          value set is higher than the value for the current set, we always move
          to the new proposed location and if the value
          is lower we move there with a probability defined by the ratio of the
          two values.
    \item Generate a random number between 0 and 1 and move to the proposed
          location in the parameter space if the number is lower than the
          probability that we calculated in the previous step. Otherwise we stay
          in the same place.
    \item Mark down the values of our current location in the parameter space.
    \item Repeat from step 2.

\end{enumerate}

After a sufficient number of repeats of this process we will end up with list of values.
These values will be representative sample of the distribution defined by the prior time
likelihood. When we divide the values by their sum we are left with estimate of the
posterior probabilities of our chosen parameter combination. With enough samples these
values give us a good estimate of the entire posterior distribution.

Why does this work? The crucial part in the algorithm is the step 3. Consider a case
where we have just one parameter that can take two different values. Now the probability
of moving from one value to the other matches the relative values of the
target distribution and that means if value A is twice as probable as value B (if we don't know the
actual probability yet), the chance of moving from A to B is 50 \% and chance of moving
from B to A is 100 \%. When we keep running process, eventually each of our two values
will be visited proportionally to its value so we would expect the value to be A in 66
\% of cases and B in 33 \% of cases. We can demonstrate this with short Python
example:

\begin{pyblock}[][fontsize=\footnotesize]
import numpy as np

np.random.seed(123)
steps = 10000
values = ["A", "B"]
sample = []
position = "A"

for i in range(steps):
    if position == "A":
        prob = np.random.random()
        if prob > 0.5:
            sample.append("B")
            position = "B"
        else:
            sample.append("A")
    else:
        sample.append("A")
        position = "A"

prob_a = sample.count("A") * 100 / steps
prob_b = sample.count("B") * 100 / steps

print(f"probability A: {prob_a} \%, probability B: {prob_b} \%")
\end{pyblock}
\stdoutpythontex
\bigskip

This logic extends to the case when we have
continuous values and also the case where we have multiple parameters. The formal proof
of this process is somewhat involved so we will skip it here, but interested readers are
advised to check chapter 7 in John Kruschke's Doing Bayesian Data Analysis\ \cite{kruschke}.

The second critical part of the algorithm is step 2 where we randomly create a new
position in the distribution where we will consider to move. In principal any kind of
random value picker works here and we will get a proper sample eventually. The problem is
that with a bad algorithm this could take a long time. What we would want to have is a
way to create proposed steps that are rejected as rarely as possible and that the
correlation between the current value and the proposed value to be as small as possible.
This would lead to quick movement over posterior distribution and more efficient sampling. Some of the
simpler solutions to this problem are still easily explainable, but
this part of the algorithm has seen some major improvement in recent years and the
current widely used solutions are technically much too complex to cover here. The
current ``industry standard'' is an an algorithm called no-u-turn-Sampler (NUTS)\ \cite{nuts}
that could be heuristically described as choosing the values by rolling a frictionless
ball on terrain defined by the inverse of our distribution while at the same time
avoiding inefficient u-turns\ \cite{kruschke}. The technicalities how this works
in practice are not for the faint of heart.

\begin{figure}[H]
    \centering
    \caption{First 100 Steps When Estimating Two Parameters With the NUTS Sampler}\label{traceexample}
    \includegraphics{trace_draw_example.pdf}
\end{figure}

To make it concrete how modern clever modern value proposal algorithms are we
have we have visualized the first 100 steps taken by the aforementioned NUTS sampler
when estimating two parameters (figure\ \ref{traceexample}). The starting value is
marked with a black X and the markers turn from red to yellow as the chain progresses.
We can see that there seem to be a clear correlation between the parameters and our
sample is already reflecting that. If we look at the sizes the steps that the algorithm
takes, we can see that these are larger in the less probable parts of the distribution.
This means that the proposed step depends on where on distribution the current position
is located. With this intelligent sampling the chain traverses more
quickly in distribution and makes the sampling much more efficient.

\subsection{How to Define the Priors?}\label{bayesproblems}

For the Bayesian formula to work we have to define a prior and this can be
trickier that it seems at first glance. Ideally the prior would codify our prior
beliefs and knowledge, but trying to rigorously codify everything we
know is often impractical, because of how complex this gets and we should aim lower as we are
anyway working with a model that is a simplified representation of reality\ \cite{gelman}. Even if
we aim for a more limited goal, codifying our existing knowledge and beliefs
to mathematic formulas can be a tricky business. It gets even more
tricky when we consider that we are doing the study not just to convince ourselves
but also other people and we should try to capture the beliefs of our target audience.
Unless this target audience is easily available for questions and is highly mathematically minded,
this leads to multiple layers of uncertainty as we try guess what others
believe and then codify these beliefs with mathematical precision.

To avoid this hurdle the other option is to try to define some non-informative
priors that ``let the data speak for itself'', but this approach also has
problems. Firstly, defining priors that have no effects on the posterior can be mathematically
very difficult when we move beyond simple models and if we would need
to invest a lot of time finding these priors we should instead probably spend this time
to define proper informed priors. Second, non-informative priors do not always
seem so non-informative, when we actually know something about the issues.
Consider a case where are trying to predict if a patient has a rare disease
and we assign a uniform prior from 0 to 1 as this seems non-informative. This
prior will have the effect of moving the posteriors towards the
possibility of the patient actually having the disease. So if we definitely know
something about the phenomenon, a non-informative prior can work against these
beliefs.\ \cite{gelman}

Considered more broadly, chasing after truly non-informative prior is also
somewhat of a fool's errand if the data actually has ``something to say''.
If the data is informative, then lots of different priors that are somewhat weak
should lead to almost identical posterior distributions. On the other hand if the data is weak and we
have a weak prior, why are we even doing the analysis if we know that it will
give us almost no information?\ \cite{gelman}

As a compromise between having strong priors based on previous beliefs and knowledge and
letting the data stand on its own, Gelman et al.
suggest using weakly informative priors\ \cite{gelman}. These can be defined in two different
ways. First we can start with non-informative prior (when it is easily
available) and then add minimum information constraining the posterior results
to reality (e.g.\ object cannot move faster than light or the height of an adult human
must be between 50 and 300 cm). The other way is to start with strong informative
priors and then broaden these to take into account the uncertainty of
how applicable our priors knowledge is to the question at hand.

Because of how complex defining priors can get, this problem can act as a limiter on
how complex Bayesian models we can build. The more complex model, the more priors we
have to define and the more prior information and beliefs we could codify and
the more difficult this problem gets. As there is no obvious universally applicable
solution to how we should create our priors, we have to make decisions case by case,
keeping in mind the existence of strong prior information, the needs of our model and
what makes sense to us and our target audience.

\section{Why Bayesian Inference Fits N-of-1 Studies}\label{whybayes}

As was mentioned before, both frequentist and Bayesian inference can be used to
estimate the parameter values of the types of models we described
in chapter\ \ref{modelling}. The choice between these two methods is not just a
technical one as both come with different conceptions about the nature of probability
and these have serious implications on how we should interpret the results and run the studies.
Unfortunately for frequentist inference, its assumptions fit the reality of running
N-of-1 trials poorly, and its results, can be hard to communicate to a lay
audience (doctors and patients). Because of these issues, Bayesian inference
can usually be recommended over frequentist inference when analyzing and
communicating the results of N-of-1 studies, even though it comes it with its'
own complexities that we previously went over.

\subsection{Flexibility of Experimental Design}\label{whybayes}

The central tenet of frequentist inference is the assumption that same the
experiment could in theory be repeated with statistically independent results. Probability
is defined in relation to these hypothetical repetitions as a proportion of them
that would have some event of interest (e.g.\ over 5 tails with 10 coin tosses).
To repeat the same experiment its design needs to be defined precisely and this
definition needs to be made before the study has begun because the changes made
to the design during the experiment might be influenced by its results.

Lets consider an experiment where we are trying to find out if a coin
is fair with frequentist methods. For this purpose, we decide to flip the coin 20 times
and count the number of heads. After this we model how ``typical'' the result we got
would be for a fair coin. We do this by calculating the distribution of heads from
multiple 20 coin toss trials
assuming that the coin is fair. Then we can compare our actual result against this
distribution. If our result seems like a typical sample from this distribution, we
can say that the null hypothesis stands and the coin is fair. This far we have no
issues. The problem comes if we did not actually decide to throw the coin 20 times
beforehand, but first threw it 10 times, looked at the results, though that we
should get some more data, and then threw the rest 10. Now what
kind of distribution should we compare our results against? The length of the trials
seems to now hinge in some undefined way from the results of the first 10 coin tosses
that made us decide to keep tossing the coin some more. For frequentist inference to
work our experiment design has to be clearly defined beforehand and we cannot make
unexpected changes to the experiment based on the data we have or any other factors.

This demand for predefined experiment designs is problematic in N-of-1 studies
used in a clinical setting. There might be cases when some treatment seems
clearly better in the middle of an experiment and the patient (or the clinician)
wants to stop the experiment. As the subjects are real people, it would be
unethical to force them to continue the study just for the sake of statistical
rigor. There might also be a situation where a couple of treatments seem
promising, but the rest seem completely useless, and we would want to change the
design of the experiment on the fly to focus just on the promising treatments.
These changes to the predefined experimental design will brake frequentist
assumptions and make the accurate inferences with this method impossible.

The option to stop the study or change the experimental design can be
incorporated to frequentist inference if we predefine the rules for when to
stop the experiment and under what circumstances the design should be altered
and how. Then we need to take these rules into account when performing the estimation
calculations which adds complexity is but is still possible. The problem is that it is
questionable if we can actually define and stick to these rules because of
the human element: we would need to convince the patient that she should
stop the experiment only in these predefined circumstances. This seems quite
unrealistic, as if there are any unforeseen problems with the treatments, it is
more than likely that the patient will want to stop regardless of any rules she
signed up to follow beforehand.

With Bayesian inference doing on the fly modifications of the study design do not
cause such problems as with frequentist inference, as there is no assumption
about repeating the experiment. Instead of hypothesizing about future
experiment, we just take into account our prior information and the data created by
the current experiment. The changes to the experimental design modify what data
we get out of it, but don't brake any assumptions of Bayesian inference. This
adaptability is a big benefit when applying N-of-1 designs in a clinical setting, as
the unpredictable nature of the patients and treatment effects and all the other
practical considerations that could force changes to our experiment design don't
break the assumptions of the inference method.

\subsection{Richness and Communicability of the Estimates}\label{whybayes}

In N-of-1 studies, much consideration has to be given to the communication of the
results, both to the patient and the clinician administering the experiment. We
would like to portray not just the estimates of the effects of the treatments,
but also the uncertainty associated with these estimates. In frequentist
inference we communicate these with maximum likelihood estimates, confidence
intervals and p-values. Maximum likelihood estimates are just point estimates and perfectly
understandable, but don't communicate any uncertainty by themselves. With confidence
intervals we can communicate uncertainty, but don't actually mark the most likely
values of the estimate. A 95 \% confidence interval is often interpreted wrongly
as including the true value of the parameter with 95 \% change, but the real
meaning is that if we repeated the experiment indefinitely 95 \% of the
95 \% confidence intervals would include the true parameter value. This is
because in frequentist interpretation of probability, true parameter values
are not random but fixed, so a single confidence interval either does or does
not include the true value. This confusing definition makes confidence intervals
a poor choice of presenting the uncertainty of the estimates to a lay audience
as they might create lots of misinterpretations and even when interpreted correctly
are hard to grasp. The second frequentist tool for communicating uncertainty
are p-values of hypothesis tests. We might define a null hypothesis that the
parameters of the effects of different treatments are equal and calculate the
likelihood to obtain the data or a more extreme version of it if this is indeed the
case. This would give us a more easily communicable way to portray the probability
to obtain the data if the null hypothesis is true. If the p-value is low we could
say with confidence that there is real differences between the effectiveness of
the treatments. The problem is that this actually gives us very little useful
information, as we would also like to how big the differences in the treatments' effectiveness
could realistically be.

A Bayesian approach to inference gives us richer and more easily communicable
information about the estimates because it gives the estimates in the form of the
posterior distribution. With this is we can easily point out the plausible values
with the tops of this distribution and the uncertainty with how flat the
distribution is. The usual misinterpretation of the confidence intervals as
containing the true value of the parameter with a certain probability is actually
the right interpretation for Bayesian credible intervals, that contain certain
percentage of the probability mass of the posterior distribution and so have a
defined chance of containing the true value of the parameter. Bayesian method itself
also seems to fit really well with the ways that clinicians conceptualize
their practice of arriving to  diagnosis for a patient as updating their beliefs
(i.e.\ priors) with the new information gained from new observation such as test
results\ \cite{clinbayes}.

\chapter{Combining Information From Several N-of-1 Trials With Hierarchical
  Models}\label{hierarchicalbayes}

Let's say we have our N-of-1 trial set up nicely and we have a good model for analyzing the
results. After running the first trials we continue to perform the same kind of setup
with multiple patients as the treatments we are considering seem to have lots of
individual variation in their effectiveness. Running these experiments separately is
fine, but one might wonder if these separate trials could somehow be combined. Of
course, we could pool the data and run a group-based
analysis about what treatments seems to be the best in general, but there is a better
way that keeps true to the goal of N-of-1 experiments of finding the best treatment for
each individual patient by using hierarchical models. In these models the
patient-level parameters are connected by imaging them as coming from a higher level
distribution. In the case of N-of-1 studies we can think of a population-level distribution
where the patient-level parameters are drawn from. This makes lots of sense as it is
clear that, for example in the case of drug effectiveness, different people have different
effects, but these effects follow a distribution so that very high or very low values are
uncommon. In a sense having a common prior for
all patient-level parameters can already be thought as defining this population level
distribution, but the difference is that, where as the prior
is set in stone, the population-level
distribution in a hierarchical model works just like the patient-level
parameters with uncertainty included and thus it is possible that our beliefs (posterior
estimates) about this distribution will be changed by the data. Just like in the case of
informative shared prior,
the estimates of this population-level distribution can draw the
patient-level estimates closer together, but as the distribution is itself is now
estimated from the data, the strength of this effect depends on how similar the
observations from different patients are.

\section{How to Make a Model Hierarchical?}

The practicalities of hierarchical models are easier to explain if we conceptualize
Bayesian
models as trees. In the top of the tree we have prior values that define the shape of
prior distribution next level down, from these distributions come the parameters that
define the data creation process at the bottom of the tree. Lets take an example
where we have \(p\) patients with \(n\) measurements and, to make things simple just
one treatment option. We can model this by a simple two-parameter model:

\begin{def}\label{}
    \begin{equation}\label{simplehierachical}
        y_{tj} = \theta_j + \epsilon_{tj}, i\in(1 \dots n), j\in(1 \dots p)
    \end{equation}
\end{def}where \(\theta_j\) represents the effect of the treatment for patient $j$ and
\(\epsilon_{tj} \sim N(0,\tau_j^2)\) is the error term for patient $j$ at observation
taken on time $t$. Now lets look at this model as a tree graph:

\bigskip
\begin{equation}\label{unpooledmodelexampletree}
\begin{tikzpicture}

    \matrix[matrix of math nodes, column sep=0pt, row sep=30pt] (mat)
    {
        \ & \mu, \sigma, \gamma & \ \\
        \theta_1, \tau_1 & \ldots & \theta_p, \tau_p \\
        y_{1,1}, \ldots, y_{n, 1} & \ldots & y_{1, p}, \ldots, y_{n, p} \\
    };

    \draw[->,>=latex] (mat-1-2) -- (mat-2-1);
    \draw[->,>=latex] (mat-1-2) -- (mat-2-3);

    \draw[->,>=latex] (mat-2-1) -- (mat-3-1);
    \draw[->,>=latex] (mat-2-3) -- (mat-3-3);

    \node[anchor=east] at ([xshift = -35pt]mat-1-1)
    {};

    \node[anchor=east] at ([xshift = -35pt]mat-2-1)
    {$\theta_j \sim \text{N}(\mu, \sigma^2), \tau_j \sim \text{HalfCauchy}(\gamma)$};

    \node[anchor=east] at ([xshift = -35pt]mat-3-1)
    {$y_{tj} \sim \text{N}(\theta_j, \tau_j^2)$};

\end{tikzpicture}
\end{equation}
\bigskip

% TODO should the variance prior always be non-informative?

For the prior distributions we chose a normal distribution for the treatment effect and half-Cauchy
distribution for the variance. The
normal distribution for the treatment effect is a natural choice as probably the
treatment effect is dependent on multiple factors which many could be independent. If
this is the case, then the central limit theorem says that the treatment effect will follow a
normal distribution. The variance parameters is more difficult and there are multiple
reasonable distributions to choose, but half-Cauchy distribution is generally
recommended for reasons we don't have space to go into here\ \cite{variancepriors}.

% TODO where is the best place to introduce logic of choosing the prior distributions?

Even though in this model each patient shares the same prior parameters, the estimates of the patient-level parameters
are only affected by the data of the patient in question as the shared prior parameter values are set
beforehand and are not affected by the data. You can think this as a model where we have
perfect knowledge of the population-level distribution and we describe this by the shape
of our priors. To change this we need to introduce
some parameter that is shared by all patients and that is also being estimated based on
the data. We can accomplish this by changing the priors from set of values to distributions
that are themselves defined by a new higher level prior values. This new level of
``uncertain priors'' are be interpreted in this context as the population-level
distribution of the treatment effectiveness, where we are uncertain about the shape of
this distribution and express this uncertainty with new higher level priors:

\bigskip
\begin{equation}\label{hierarhicalmodelexampletree}
\begin{tikzpicture}

    \matrix[matrix of math nodes, column sep=0pt, row sep=30pt] (mat)
    {
        \ & \mu_{\mu}, \sigma_{\mu}, \gamma_{\sigma} & \ \\
        \ & \mu, \sigma & \gamma \\
        \theta_1, \tau_1 & \ldots & \theta_j, \tau_j \\
        y_{1,1}, \ldots, y_{n, 1} & \ldots & y_{1, p}, \ldots, y_{n, p} \\
    };

    \draw[->,>=latex] (mat-1-2) -- (mat-2-2);
    \draw[->,>=latex] (mat-2-2) -- (mat-3-1);
    \draw[->,>=latex] (mat-2-2) -- (mat-3-3);
    \draw[->,>=latex] (mat-2-3) -- (mat-3-1);
    \draw[->,>=latex] (mat-2-3) -- (mat-3-3);
    \draw[->,>=latex] (mat-3-1) -- (mat-4-1);
    \draw[->,>=latex] (mat-3-3) -- (mat-4-3);

    \node[anchor=east] at ([xshift = -40pt]mat-2-2)
    {$\mu \sim \text{N}(\mu_{\mu}, \sigma_{\mu}^2), \sigma \sim
    \text{HalfCauchy}(0, \gamma_{\sigma})$};
   
    \node[anchor=east] at ([xshift = -40pt]mat-3-1)
    {$\theta_j \sim \text{N}(\mu, \sigma^2), \tau_j \sim \text{HalfCauchy}(0, \gamma)$};

    \node[anchor=east] at ([xshift = -40pt]mat-4-1)
    {$y_{ij} \sim \text{N}(\theta_j, \tau_j^2)$};

\end{tikzpicture}
\end{equation}
\bigskip

Now instead of drawing the treatment related patient-level parameters straight from a distributions defined
by the priors, these are drawn from the population-level distributions. The parameters
of the
population-level distributions themselves are taken from the prior values expressing our
uncertain knowledge about these distribution. The exception to this pattern is the error
term \(\tau \). We could make this
parameter hierarchical, but this is not commonly done in hierarchical models. There are
two reasons for this. Firstly this term is already the ``extra'' in the model describing
the variance left after the more interesting parameters have been fitted. Compared to the actual
treatment effect this makes it less interesting and adding hierarchical layers for this
parameters can be thought of as unnecessary complexity. The second reason is that with a
reasonable amount of observations, the error term can be estimated very accurately. This
means that adding the hierarchy should not have much effect on this estimate as the
patient-level estimates are likely too strong to be swayed by the population-level
estimates.

% TODO why the variance can be estimated so accurately

\section{Benefits of Hierarchical Models}

% TODO check that e.g. and i.e. are used appropriately
% TODO check spelling of patient-level and population-level

The kinds of multilayered dependencies introduced by hierarchical models are useful in many ways.
Firstly, the dependencies can meaningful for the application, i.e.\ we can
model the treatment effect for a single patient as an instance from a
broader population distribution of this effectiveness. The population-level is not just
a mathematical curiosity, but it has a meaningful interpretation and the estimates we
get for the population-level parameters are interesting by themselves. Secondly, because
of the dependencies across parameters, all the data can now jointly inform all the
parameter estimates. As there is now more data used for the lower-level estimates trough
pooling, there is usually some reduction in the variance of these estimates
and especially between them. This reduction of variance between the estimates is
generally referred by the term ``shrinkage''. In general,
shrinkage in hierarchical models causes lower-level parameters to shift toward the modes
of the higher-level distribution. This does not mean that
estimates are always just pulled closer together, because if the higher-level
distribution has multiple modes, then the low-level parameter values cluster
more tightly around these modes, which might pull some
low-level parameter estimates apart instead of together. The greatest thing
is, that as we don't explicitly set the parameter values of the higher-level
distributions, the amount of shrinkage is informed by the data so that
similar observed data points from lower-level distributions lead to ``tighter''
estimates for the higher-level distributions and in this in turn leads to
greater shrinkage. This means that using a hierarchical model in the case where the
patients are actually are very different, does not force the estimates together as
widely spread measurement values leads to the estimates of the population level-distributions to
have a large variance and thus to have little impact on the patient level estimates. So
instead forcing similarity, the model ``makes the decisions'' if this is appropriate
based on the data (and our priors, as strong priors might force or prevent this effect).

\section{Applying the Bayes Formula to Hierarchical Models}

With all the parameters introduced by the new parameter levels in hierarchical models,
the question now becomes, how we apply
the Bayesian formula in these cases? Luckily this happens to be very simple as we just
have to remember two rules of probability: the chain rule\ \ref{chainrule}, that
tells us that the probability of \(A\) and \(B\) is the probability of \(B\) on condition \(A\)
times the probability of \(A\), and the rule of independence\ \ref{ruleofindependence}
that states that the
probability of \(A\) on condition \(B\) is just the probability of \(A\) if \(A\) and
\(B\) are independent.

\begin{equation}\label{chainrule}
    P(A \cap B) = P(B|A)P(A)
\end{equation}

\begin{equation}\label{ruleofindependence}
    P(A|B) = P(A), \text{ if } P(A \cap B) = P(A)P(B)
\end{equation}
\smallskip

Now lets see what happens when we apply write are our previously defined hierarchical
model\ \ref{hierarhicalmodelexampletree} in the Bayes formula\
\ref{bayeswithparams} and apply these rules:

\begin{equation*}\label{}
    \begin{aligned}\label{conjugatebayesrule}
        &P(\theta_1,\tau_1,\theta_2,\tau_2,\mu,\sigma | Y) \\
        &= \frac{P(Y | \theta_1,\tau_1,\theta_2,\tau_2,\mu,\sigma)P(\theta_1,\tau_1,\theta_2,\tau_2,\mu,\sigma)}{P(Y)} \\
        &\text{(dropping the constant term)} \\
        &\propto P(Y | \theta_1,\tau_1,\theta_2,\tau_2,\mu,\sigma)P(\theta_1,\tau_1,\theta_2,\tau_2,\mu,\sigma) \\
        &\text{(using rule of independence as data Y depends only on parameters $\theta$ and $\tau$)} \\
        &= P(Y | \theta_1,\tau_1,\theta_2,\tau_2)P(\theta_1,\tau_1,\theta_2,\tau_2,\mu,\sigma) \\
        &\text{(using rule of independence to separate the two patients)} \\
        &= P(Y_1 | \theta_1,\tau_1)P(Y_2 | \theta_2,\tau_2)P(\theta_1,\tau_1,\theta_2,\tau_2,\mu,\sigma) \\
        &\text{(using the chain rule to separate the patient and population parameters)} \\
        &= P(Y_1|\theta_1,\tau_1)(Y_2|\theta_2, \tau_2)P(\theta_1,\tau_1|\mu,\sigma)P(\theta_2,\tau_2|\mu,\sigma)P(\mu, \sigma)^2 \\
        &\text{(population parameters are independent of each other)} \nonumber \\
        &= P(Y_1|\theta_1,\tau_1)(Y_2|\theta_2, \tau_2)P(\theta_1,\tau_1|\mu,\sigma)P(\theta_2,\tau_2|\mu,\sigma)P(\mu)^2P(\sigma)^2 \\
        &\text{($\theta$ and $\tau$ are independent on each other and $\tau$ is not dependent on} \\
        & \text{population level parameters)} \\
        &= P(Y_1|\theta_1,\tau_1)(Y_2|\theta_2, \tau_2)P(\theta_1|\mu,\sigma)P(\tau_1)P(\theta_2|\mu,\sigma)P(\tau_2)P(\mu)^2P(\sigma)^2 \nonumber
    \end{aligned}
\end{equation*}

We have now successfully expressed our hierarchical model in the Bayes formula.
We can see that the measurements of each patient \(Y_1, Y_2\) depend directly only on the patient
level parameters \(\theta_1, \theta_2, \tau_1, \tau_2\), so that if these values are set, then the
measurements are independent of all other parameters. Similarly, \(\theta_1, \theta_2 \) depend conditionally only
on the population level parameters \(\mu, \sigma \), while \(\tau \) is independent of
them. We could continue by writing the
formula out by adding in the prior terms and replacing the abstract probability
markings with the actual probability density functions given by our chosen
distributions, but this gets messy and, in this case, no nice analytical solution can be
found anyway, so we will leave it at this. It is notable that we could add even
more layers to the model and just apply the same rules we used here to write the formula
out. Bayes formula makes no limitations on how complex and deep our hierarchical model
can be.

% TODO what else do I need to add to this chapter?

\chapter{Example of a Hierarchical Bayesian Analysis Using Simulated Data}

To put everything we learned into practice, lets go over an example Bayesian
analysis of multiple N-of-1 trials, where we first analyze one trial separately and then
combine multiple trials by using a hierarchical model. For the analysis we use Python and PyMC3 library\ \cite{pymc3} built
for probabilistic programming. We will not go over each part of the code used, but focus only
on the critical parts where we see how the principals we went over in the
previous chapters look like when implemented in practice. For those who want to see the
full code, it is available from a GitHub repository\ \cite{github} with instructions on how
to run it.

As we now jump from theoretical and analytical considerations to practical consideration
when doing this kind of analysis, we will need to revisit the concepts that we went over
while talking about the problems of dealing with marginal likelihood\
\ref{marginallikehoodproblems} and introduce some new concepts related to how
PyMC3 deals with this problem and what we need to pay attention to
make sure that the algorithmically derived results we get are reliable. We also have to
consider how we can make sure that our model is ``good enough'' description of reality.
Almost any kind of model can be fitted to the data and most of these can be even
estimated correctly, but only few models describe the data generating process in an
accurate enough manner. To answer this question we need to introduce totally new concepts
relating posterior predictive checks, where we create new data based on the estimated
posteriors and see if this data looks like our original dataset.

\section{Defining the Experiment}

Lets first describe our experiment design, as this is critical for understanding
everything that comes later. Imagine that we have 6 patients that all suffer from the
same medical condition to which there are two treatment options available (\(A\)) and
(\(B\)). We know from the
literature that there is clear patient-level variation in which treatment works better
so we decide to test this with N-of-1 trials. We know that both of the treatments act
very fast and that there are no known problems with interaction effects between them.
The state of the condition can also be easily measured at home so no visits to
a hospital are needed to take the measurements. Higher measurements mean worse condition
so we would like to find the treatment that gives the lowest measurements for each patient. The condition is usually steadily
worsening with or without the treatment and the treatment is more for slowing down the
disease and not curing it. Based on this information we come up with the following experiment design:

\begin{itemize}
    \item The trial will run for 4 weeks with 2 weeks on each treatment
    \item Treatment periods will be one week long
    \item The week-long treatment periods will be organized into two blocks, each of which will run for
    2 weeks and contain 1 week on each treatment
    \item The sequence of the treatments in the first block will be randomized and then
    the second block will have the treatments in reverse order creating a balanced
    design (ABBA or BAAB)
    \item Measurements will be taken by the patient at the end of each day so we end up with
    28 measurements per patient with 14 for each of the treatments that are separated by
    roughly equal time-periods (one day)
    \item There will be no washout period when switching between the treatments
\end{itemize}

\begin{figure}[H]
    \centering
    \caption{Experiment Design for a Single Patient}\label{singlepatientexperimentdesign}
    \includegraphics{design_for_simulated_experiment.pdf}
\end{figure}

\section{Simulating the Data}

Next we simulate the data that we could get from our hypothetical experiment with Python.
As there should not be any interaction effects between the treatments, the treatment
effect onset is immediate and there is no washout period, this is a fairly simple
task. The benefit of having a relatively simple data generating process, is that it is
easier to compare the estimated results from our models with the actual parameters. We will not go
over the code that creates the data in detail, but concentrate on
the mathematical model that it is based on. The full code can be found in the GitHub repository
\cite{github}. For simulating each observation at time \(t\) for patient \(j\) we use the following model:

\begin{def}\label{simulationmodel}
    \begin{equation}\label{}
        y_{tj} = Z_{tj}\theta_{j} + W_{tj}\eta_{j} + t\beta_j + \epsilon_t + \rho_j\epsilon_{t-1},
    \end{equation}
\end{def}where \(\theta_j\) and \(\mu_j\) are the treatment effects with \(Z_{tj}\) and \(W_{tj}\) being indicator
variables that are 1 when observation \(t\) is within a period where treatment was applied and
0 otherwise. \(\beta_j\) is the trend representing the natural progress of the
condition. \(\epsilon_{tj} \sim N(0,\tau_j^2) \) is the error term at \(t\) and
\(\rho_j\) is the correlation between consecutive errors.

This data generating model has both a trend and a lag-1-autocorrelation. The trend we
are going to be modeling later, but also modeling autocorrelation would make the model
too complex for the limited amount observations we will be dealing with. This way the example tries to be more realistic
by having the data generation function to be of such complexity that it cannot be
fully modeled and some decisions have to be made on how to try to approximate
the process with a simplified version of reality.

``Above'' the part creating the individual observations we have a population-level parameters
that define the distribution from which the patient-level parameters are drawn from.
Here we are mimicking a hierarchical model also in the data creation process. This is
important because if there was no underlying pattern to the patient-level parameter values
, pooling the information across the patients
would be nonsensical. So instead of directly picking the patient-level parameter values,
we are just going to pick the form of the distribution where these are drawn from.

\bigskip
\begin{tikzpicture}

    \matrix[matrix of math nodes, column sep=-70pt, row sep=30pt] (mat)
    {
    \ & \mu_{\theta}, \sigma_{\theta}, \mu_{\eta},\sigma_{\eta}, \mu_{\beta},
    \sigma_{\beta}, \gamma_{\tau}, \alpha_{\rho}, \beta_{\rho} & \ \\
        \theta_1, \eta_1, \beta_1, \tau_1, \rho_1 & \ldots & \theta_6, \eta_6, \beta_6, \tau_6, \rho_6 & \\
        \ & \ & \ \\
        y_{1,1} \ldots, y_{28, 1} & \ldots & y_{1,6} \ldots, y_{28, 6} \\
    };

    \draw[->,>=latex] (mat-1-2) -- (mat-2-1);
    \draw[->,>=latex] (mat-1-2) -- (mat-2-3);
    \draw[->,>=latex] (mat-2-1) -- (mat-4-1);
    \draw[->,>=latex] (mat-2-3) -- (mat-4-3);

    \node[anchor=east] at ([xshift = -40pt]mat-2-1)
    {$
        \theta_j \sim \mathcal{N}(\mu_{\theta},\sigma_{\theta}^2),
        \eta_j \sim \mathcal{N}(\mu_{\eta},\sigma_{\eta}^2),
        \beta_j \sim \mathcal{N}(\mu_{\beta},\sigma_{\beta}^2),
    $};
    \node[anchor=east] at ([xshift = -40pt]mat-3-1)
    {$
        \tau_j \sim HalfCauchy(0, \gamma_{\tau}),
        \rho_j \sim Beta(\alpha_{\rho}, \beta_{\rho})
    $};
    \node[anchor=east] at ([xshift = -40pt]mat-4-1)
    {$
        y_{tj} = Z_{tj}\theta_{j} + W_{tj}\eta_{j} + t\beta_j + \epsilon_t + \rho_j\epsilon_{t-1}
    $};

\end{tikzpicture}
\bigskip

% TODO is half-cauchy the right choice for sd or var?

To create the simulated data, we set the population parameters to the values described
in table\ \ref{populationparameters} and then run
our data creation process:

\begin{table}[H]
    \caption{Population Level Parameter Values for the Simulated Data}\label{populationparameters}
    \begin{align}\label{}
        \text{population treatment A mean: } & \mu_{\theta} = 10 \nonumber \\
        \text{population treatment A standard deviation: } & \sigma_{\theta} = 0.2 \nonumber \\
        \text{population treatment B mean: } & \mu_{\eta} = 9.7 \nonumber \\
        \text{population treatment B standard deviation: } & \sigma_{\eta} = 0.3 \nonumber \\
        \text{population trend mean: } & \mu_{\beta} = 0.02 \nonumber \\
        \text{population trend standard error: } & \sigma_{\beta} = 0.01 \nonumber \\
        \text{population measurement error scale: } & \gamma_{\tau} = 0.3 \nonumber \\
        \text{population error autocorrelation alpha: } & \alpha_{\rho} = 100 \nonumber \\
        \text{population error autocorrelation beta: } & \beta_{\rho} = 200 \nonumber
    \end{align}
\end{table}

There are some important things to note about these values. First, we defined the
treatment \(A\) to usually have higher mean than treatment \(B\) meaning that the
treatment \(B\) should perform better (lower measurement values) for most patients.
Second, the mean of the trend \(\beta \) is positive and its variance is small. This means
that almost all patient should have a steadily worsening condition (measurements get
higher as the experiment progresses).

% TODO is the variance of the treatment effects too small?

In figure\ \ref{measurementtimeline} we see timelines of the simulated measurements for each patient. Just by looking, it seems
pretty obvious that for patients 2 and 1 there is a clear difference in the efficacy of the
treatments, with the treatment periods clearly visible. We also see more ambivalent
cases. Patient 4 seems to have very stable measurement values regardless of the
treatment, while patient 2 has lots of random fluctuations in the data making it hard to
interpret the data just by looking at the plot. We can also see the upwards trend in the
measurements, as all patients except patient 2 end the experiment with higher measurement values
that they started with (remember that because of the balanced design, patients start and
end the experiment with the same treatment).

\begin{figure}[H]
    \caption{Timeline of Measurements for Each Patient}\label{measurementtimeline}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{measurements_timeline.pdf}
\end{figure}

% TODO use colors or patterns that are easier to discern

Let's next look at the actual parameters that created this data.
In figure\ \ref{parameterdistribution}, we have all patient-level parameter values
overlayed on top of the population
distribution from which they where drawn from. On the left side we can see both of the
treatment effect means. These are plotted on an identical x-axis for easy comparison.
We can see that for every patient, except for patient 3, treatment B is
better (lower parameter values). The second interesting finding is that for patient
1 the treatment B effectiveness is far better than for other patients. When we look later on at
the hierarchical model, these two patients are the ones to keep an eye out, as we would
expect the measurements from other patients to pull the treatment effect of
treatment B for patient 3 down (make it better) and conversely pull this up for patient
1. From the measurement error and autocorrelation there are less interesting things to
note: the wild swings of the measurements for patient 2
are explained by her high measurement error and autocorrelation values are quite tightly
grouped up for all patients. For the trend we see that patients 2, 4 and 5 are getting
worse significantly slower than the other patients.

\begin{figure}[H]
    \caption{Parameter Used to Create Simulated Data}\label{parameterdistribution}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{population_parameter_distributions.pdf}
\end{figure}

\section{Analyzing a Single Trial}\label{1trial}

We start analyzing our simulated experiments by first focusing on just a single patient,
choosing naturally patient 1. In figure\ \ref{patient1timeline} we have the timeline of
observations for this patient. We have highlighted the period where the patient was on treatment A with grey and can
clearly see that this treatment is the inferior option for this patient, so modelling
this we should expect to get the same recommendation, but with more specificity.

\begin{figure}[H]
    \caption{Timeline of Measurements for Patient 1}\label{patient1timeline}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{patient1_timeline.pdf}
\end{figure}

Normally before we would start with a more extensive exploration of the data, but as teaching preliminary analysis
is not the focus here and we have created the data ourself and so know exactly what kind
of distribution it follows and that there are not missing values or outliers, we will
skip this step here. For analyzing any real-world data this proper data exploration would be \emph{mandatory}.

\subsection{Defining the Model}

We use a model with the two treatment parameters and error term and a trend\ \ref{singlepatientmodel}. We could
start with a more simple model without the trend, but as we saw from figure\ \ref{patient1timeline}
there is a clearly trend in the data, so adding this parameter from the start seem appropriate:

\begin{def}\label{}
    \begin{equation}\label{singlepatientmodel}
        y_t = Z_t\theta + W_t\eta + t\beta + \epsilon_t
    \end{equation}
\end{def} where \(\theta \) and \(\eta \) are the treatment effects with \(Z_t\) and \(W_t\) being indicator
variables that are 1 when observation \(t\) is within a period where treatment was applied and
0 otherwise. \(\beta \) is the trend and \(\epsilon_{t} \sim N(0,\tau^2) \) is the error term at \(t\).

Our model assumes that the measurement values are distributed normally with
variance \(\tau^2\) and a mean of either \(\theta \) or \(\eta \) depending on which
treatment is applied at the observation \(y_t \sim N(Z_t\theta + W_t\eta, \tau^2)\).
This means that our likelihood function will have to be a bit special as using a simple
normal distribution will not do. We will see that this will cause use some headaches
later when we implement the model in PyMC3.

Next we go to choose the prior distributions for our model. We choose
normal distributions for the treatment effect parameters and the trend as it makes sense to think
that these would depends on multiple independent biological and social
factors and the central limit theorem tells us the that
the parameters should then be normally distributed. For the variance of the error
term we choose a half-Cauchy distribution as this is a common recommendation for the
prior of variance\ \cite{variancepriors}.

\bigskip
\begin{tikzpicture}

    \matrix[matrix of math nodes, column sep=0pt, row sep=30pt] (mat)
    {
        \mu_{\theta}, \sigma_{\theta}, \mu_{\eta}, \sigma_{\eta}, \mu_{\beta}, \sigma_{\beta}, \gamma_{\tau} \\
        \theta, \eta, \beta, \tau \\
        \ \\
        y_{1} \ldots, y_{28} \\
    };

    \draw[->,>=latex] (mat-1-1) -- (mat-2-1);
    \draw[->,>=latex] (mat-2-1) -- (mat-4-1);

    \node[anchor=east] at ([xshift = -40pt]mat-2-1)
    {$
        \theta \sim N(\mu_{\theta}, \sigma^2_{\theta}),
        \eta \sim N(\mu_{\eta}, \sigma^2_{\eta}),
    $};
    \node[anchor=east] at ([xshift = -40pt]mat-3-1)
    {$
        \beta \sim N(\mu_{\beta}, \sigma^2_{\beta}),
        \tau \sim HalfCauchy(0, \gamma_{\tau})
    $};
    \node[anchor=east] at ([xshift = -40pt]mat-4-1)
    {$
        y_t = Z_t\theta + W_t\eta + t\beta + \epsilon_t
    $};

\end{tikzpicture}
\bigskip

% TODO should we use a separate scale parameter so that we would not have to worry
% about combining this info in the variance parameter?

Now that we have our model defined we should think about what kind of values we should
give to our prior parameters. Lets say that for treatment A and B we know from the
literature that the typical measurements taken under these treatments are around 10. We
can just slot this value in for the mean prior parameter values. Same thing can be
easily applied to standard deviance as we can just slot in the standard deviance for the
patient treatment effect mean taken from existing studies. Lets say that this value is
0.5, so that it is rare to values above 11.5 or under 8.5. We can tone down this certainty
as we are not very sure that the patients on which the previous studies where conducted
are that similar to our patients. We do this by doubling the standard deviance prior to 1 for both
treatment options. For the trend we are not so certain, but we assume that it should be
increasing slightly, so we set the mean to 0.1. Because of our uncertainty we set the
standard deviance to 0.3, giving this value a lot of flexibility and allowing the trend to be also
positive without the patient being a huge outlier. For the scale of the half-Cauchy
distribution we go with a non-informative prior as the error also depends on if on
properly the measurement is performed (remember that the measurements are taken by the
patients) and this is very hard to estimate so we set the value \(\gamma \) value to 10
that gives us a very broad believable value range. The set prior values can be seen
in table\ \ref{singlelpatientmodelpriorvalues}.

\begin{table}[H]
    \caption{Single Patient Model Prior Values}\label{singlelpatientmodelpriorvalues}
    \begin{align}\label{}
        \text{treatment A mean prior: } & \mu_{\theta} = 10 \nonumber \\
        \text{treatment A standard deviance prior: } & \sigma_{\theta} = 1 \nonumber \\
        \text{treatment B mean prior: } & \mu_{\eta} = 10 \nonumber \\
        \text{treatment B standard deviance prior: } & \sigma_{\eta} = 1 \nonumber \\
        \text{trend mean prior: } & \mu_{\beta} = 0.1 \nonumber \\
        \text{trend standard deviance prior: } & \sigma_{\beta} = 0.3 \nonumber \\
        \text{measurement error scale prior: } & \gamma_{\tau} = 10 \nonumber
    \end{align}
\end{table}

\subsection{Implementing the Model in PyMC3}

PyMC3 is Python package built for probabilistic programming. Although it is not
limited to Bayesian methods, that is its main focus. Implementing Bayesian models in
the package works so that you give it all the parts of the Bayesian formula except the
marginal likelihood: the likelihood, prior
distribution and the values defining the prior distributions. Then it automatically
chooses a Markov Chain Monte Carlo method (MCMC) that it thinks can most efficiently
algorithmically approximate the posterior distribution. This is very nice, as this takes out a
huge chunk of work from us, namely choosing and implementing the approximation method
ourself. But now that we have entered the territory of algorithmic solutions, we have
introduced a new hurdle, namely checking that the algorithm is actually works for our
model and gives us good approximations. On top of this we should check that our model is a good
approximation of the reality. We will go over how these problems manifest themselves and
how we can deal with them when these problems pop up, so lets us just jump in and
implement the model in PyMC3 and then walk trough what the code is
doing line by line\footnote{The code snippets that we will be showing are a
slight simplification of the actual code that was used to calculate the results. The simplification does not mean
that the code shown is not executable, but parts of it that are related to technical niceties
(e.g. inter OS compatibility, saving the plots\ldots) are taken out to focus on what is
most important}.

% TODO the last to thing to edit here should be that the following
% code does not get obscured by the footnote
\pagebreak

\subsubsection*{Full Model Implementation in PyMC3}
\bigskip

\begin{pyverbatim}[][fontsize=\footnotesize]
# load in needed packages
import pymc3 as pm
import pandas as pd

# read measurement data and the true parameter values
measurements_df = pd.read_csv("patient_measurements.csv")
parameters_df = pd.read_csv("patient_parameters.csv")

# create convenience index for easy filtering
patient_idx = measurements_df["patient"]
   
# define the model
with pm.Model() as single_patient_no_trend_model:

    # defining our prior beliefs of the distributions of parameters
    treatment_a = pm.Normal("Treatment A", mu=10, sigma=1)
    treatment_b = pm.Normal("Treatment B", mu=10, sigma=1)
    trend = pm.Normal("Trend", mu=0.1, sigma=0.3)
    gamma = pm.HalfCauchy("Gamma", beta=10)

    # creating a vector defining the expected means of the observations
    # which are defined by one of the treatment means + trend
    measurement_means = (
        treatment_a * measurements_df[patient_idx == 0]["treatment_a"]
        + treatment_b * measurements_df[patient_idx == 0]["treatment_b"]
        + trend * measurements_df[patient_idx == 0]["measurement"]
    )

    # likelihood is normal distribution with the same amount of dimensions
    # as the patient has measurements with means defined by the previously
    # defined vector and all dimensions share the same variance
    likelihood = pm.Normal(
        "y",
        measurement_means,
        sigma=gamma,
        observed=measurements_df[patient_idx == 0]["value"],
    )

    # adding the comparison between the treatments for monitoring
    difference = pm.Deterministic(
        "Treatment Difference (A-B)", treatment_a - treatment_b
    )

    # running the algorithmic estimation process
    trace = pm.sample(800, tune=500, cores=3)

    # Checking diagnostic describing how the algorithm performed
    pm.traceplot(
        trace, ["Treatment A", "Treatment B", "Trend", "Gamma"], divergences="top"
    )
    plt.show()
    pm.summary(trace)

    # checking the posterior distributions
    pm.plot_posterior(trace)
    plt.show()

 
# creating new observations based on our model with sampled
# posterior estimates (posterior sampling)
with single_patient_no_trend_model as model:
    post_pred = pm.sample_posterior_predictive(trace, samples=500)
    predictions = post_pred["y"]

# visualizing posterior samples to check the model's match with reality
draw_posterior_checks(
    predictions=predictions,
    measurements_df=measurements_df[patient_idx == 0],
    parameters_df=parameters_df[parameters_df["patient"] == 0],
)
\end{pyverbatim}
\smallskip

\subsubsection*{Part 1: Loading in the Package and the Data}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
import pymc3 as pm
import pandas as pd

measurements_df = pd.read_csv("patient_measurements.csv")
parameters_df = pd.read_csv("patient_parameters.csv")

patient_idx = measurements_df["patient"]
\end{pyverbatim}
\smallskip

The first thing we of course have to do is to load the packages and get our data. There is
not much to comment about loading the packages, but lets see how our data in
\pyv{measurements_df} looks like:

\bigskip
\begin{table}[H]
\caption{Structure of the Measurement Data}\label{measurements}
\bigskip
\centering
\begin{pycode}
import os
import pandas as pd

measurements_path = os.path.join("..", "data", "patient_measurements.csv")
measurements_df = pd.read_csv(measurements_path)
measurements_df.columns = ['patient', 'measurement', 'period', 'block',
'treatment_a', 'treatment_b', 'value']
print(measurements_df.head(10).to_latex(index=False))
\end{pycode}
\end{table}
\bigskip

All columns expect the last one are index columns. As is the standard in Python, everything
zero-indexed. First column indexes the patient (0-5), second the
measurement (0-27), third the treatment period (0-3), fourth the block (0-1) and fifth
and sixth which treatment was applied during the particular measurement (0-1). Last
column is contains the values of the measurements. This format was chosen just for
convenience as PyMC3 does not require any particular format from the
data as you can feed the right values into
the right places in the model definition. Also using the Pandas package to handle the
data as a dataframe is not necessary, but is very convenient.

The \pyv{parameters_df} dataframe shown in table\ \ref{parameters} contains the true parameter values of the patients. These
can be used check how accurate our predictions actually are. Normally we would of course
not have access to these values, but as we created the data ourself, we do. Each row in
this dataset is a patient and contains the
true parameter values for each of the parameters we used to create the simulated data.
Only a couple of the columns included in the dataset are shown here, because the full
table would not fit.

\bigskip
\begin{table}[H]
\caption{Structure of the Parameter Data}\label{parameters}
\bigskip
\centering
\begin{pycode}
import os
import pandas as pd

parameters_path = os.path.join("..", "data", "patient_parameters.csv")
parameters_df = pd.read_csv(parameters_path)
parameters_df.drop(columns=["treatment_order", 'autocorrelation'], inplace=True)
parameters_df.columns = ['patient', 'treatment_a', 'treatment_b', 'trend', 'measurement_error_sd']
print(parameters_df.to_latex(index=False))
\end{pycode}
\end{table}
\smallskip

The last part of the code where we create the variable \pyv{patient_idx} is just for
convenience so that we need to type less when referring to the data of a particular patient.

\subsubsection*{Part 2: Initializing the Model}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
with pm.Model() as single_patient_no_trend_model:
\end{pyverbatim}
\smallskip

PyMC3 models are usually defined within the Python context manager (``with'' statements).
This is not necessary, but makes the model definition much cleaner as all parts
defined within the context (indented parts of the case), are automatically associated
with this particular model without need for manually defining this for every part.

\subsubsection*{Part 3: Defining the Priors}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
treatment_a = pm.Normal("Treatment A", mu=10, sigma=1)
treatment_b = pm.Normal("Treatment B", mu=10, sigma=1)
trend = pm.Normal("Trend", mu=0.1, sigma=0.3)
gamma = pm.HalfCauchy("Gamma", beta=10)
\end{pyverbatim}
\smallskip

Now we start getting to the actual modeling. We start by defining our prior beliefs about
out the parameter distributions. For both treatments and the trend we use a normal distribution and
for the variance of the error term \(\gamma \) we use a half-Cauchy. For the half-Cauchy
distribution we don't need to define the location term as it is by default zero, just
like we want it. Something to note here is that the PyMC3 normal distribution sigma
parameters expects the standard deviation and not the variance. As all the distributions
that we are using here are well defined and common, we can find functions implementing
them already built-in PyMC3. It is also possible to define completely custom
distributions, but unfortunately this is quite complex, as you don't just have to worry
about the math part, but also all you have to implement all the methods that PyMC3
relies on when making the calculations\footnote{Because of the complexity of defining
custom distributions the author would recommend using the competing PyStan package in
cases where inbuilt distributions don't exist. Although Stan requires to you to write
model definition with its own language, this hurdle is worth in some cases as you just have
worry about the math and there is no need to consider implementing any helper functions}.

\subsubsection*{Part 4: Defining the Likelihood}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
measurement_means = (
    treatment_a
    * measurements_df[patient_idx == 0]["treatment_a"]
    + treatment_b
    * measurements_df[patient_idx == 0]["treatment_b"]
    + trend
    * measurements_df[patient_idx == 0]["measurement"]
)

likelihood = pm.Normal(
    "y",
    measurement_means,
    sigma=gamma,
    observed=measurements_df[patient_idx == 0]["value"],
)
\end{pyverbatim}
\smallskip

Defining the likelihood is the most difficult part our model definition because of the
complexity introduced by the varying treatments. Because of this, the likelihood cannot
be implemented as a simple normal distribution. We solve this problem by instead using a
multivariate normal distribution with as many dimensions as the patient has
measurements. We construct the vector of means for this distribution by multiplying the
treatment distributions by the treatment indices (1 when
treatment given, 0 when not) and add the trend multiplied by the index of the
measurement (\(t\)). So instead of considering
the observations as multiple samples from the same distribution, we think of them as a
single multidimensional sample from
a multivariate normal distribution. This is not the only possible way to solve the
problem, but is probably the most convenient. If we would have some
dependencies between the measurements in our model (e.g. autocorrelation), this approach
would not work. In this case we would have to implement the likelihood function
ourselves. One thing to note here is that even though we are using a multivariate normal
distribution, we still use the same PyMC3 function for normal distribution and just give
it a vector of means that matches the amount of our observations. This strategy works
with all the inbuilt PyMC3 distributions: you can either have them use a single
parameter values or a vector of values matching the dimensions of the data and PyMC3 will automatically
change from using a single dimensional distribution with multiple observations to
multidimensional distribution with one observation vector.

\subsubsection*{Part 5: Defining the Parameter Comparison}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
    difference = pm.Deterministic(
        "Treatment Difference (A-B)", treatment_a - treatment_b
    )
\end{pyverbatim}
\smallskip

As the estimate we are most interested is not actually any individual parameter, but the
difference between the two treatment parameters, we have to add this comparison separately.
We can do this before or after running our model, but here we have decided to do it
beforehand. As the difference is fully determined by the values of the treatment
parameters and not random, when conditioning on these values, we have to use PyMC3
pm.deterministic function that calculates the posterior
by simple subtracting the value of treatment 2 parameter value from treatment 1
parameter value at each step in the chain.

\subsubsection*{Part 6: Running the Model}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
trace = pm.sample(draw=800, tune=500, cores=3)
\end{pyverbatim}
\smallskip

Now that we have defined our prior beliefs and our likelihood, we can move on to running the
model. PyMC3 will now estimate the posterior with an automatically chosen Markov Chain
Monte Carlo (MCMC) method (see chapter\ \ref{bayesinferencechallenges})
that samples the distribution defined by likelihood times priors and estimates the
posterior based on these samples. Instead of relying on PyMC3s automatic selection, we could also manually choose what
algorithm to use, but unless we see problems when we evaluate the estimation efficiency,
we should stick with the default option.

The three parameters that we have to manually define are as follow:

\begin{itemize}
    \item[] draw = how many steps the MCMC chain should calculate
    \item[] tune = how many steps the MCMC chain should take before it starts to
    take the observations into account
    \item[] cores = how many MCMC chains we will calculate in parallel
\end{itemize}

The total steps taken by each chain is draw+tune. The reason for the tuning steps is
that the starting position of the chain is probably biased. We would want the starting
position to be a sample from the posterior so that its probability of it being in a certain
point would be defined by the posterior. We can do this by letting the chain first take
a number of steps and as its moves trough the posterior its location distribution will start to
reflect the actual posterior. If we would not have the tuning steps it would be possible
that the chain starts in a weird position and the first steps it takes don't reflect the
posterior as it might have to make lots of moves in a certain direction to get to the
representative parts of the distribution. In principal we could use the tuning steps
also for the calculation as long as we have enough steps in total end product would
still converge to the same distribution, but it is more efficient to just drop these
values so that we don't have add more steps to compensate the possible bias introduced by
them. The more draws and tuning steps we take the better the estimation will be, but the
more time the calculation will take. Finding the
right number of draw and tuning steps involves usually some trial and error. It is
hard to know how many steps you need before you try to run the model. Luckily PyMC3 will
automatically alert us if it thinks that we need more tuning steps or draws and we can
also diagnose this from the metrics and plots that we will look at after we have run the
model.

% TODO have we used the word chain previously?

As modern CPUs have usually multiple cores, we can speed up the calculation by running
multiple chains in parallel. Each chain is independent of each other and will
take same amount of draws and tuning steps. As the chains can runs in parallel, having
multiple chains running can dramatically speed up our estimation. The total number of steps that
the posterior will be calculated with is the combined draws (not tuning steps) of the
chains. So in our case we will use 3 cores and have 3 chains (running this on a 4 core
CPU so we will leave 1 core free for other tasks), these chains will make in total (800 +
700) * 3 = 4500 steps, of which 800 * 3 = 2400 steps will be used for the posterior estimation.

\subsubsection*{Part 7: Checking Algorithm Diagnostics}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
# Checking diagnostic describing how the MCMC method performed
pm.traceplot(
        trace, ["Treatment A", "Treatment B", "Trend", "Gamma"], divergences="top"
    )
plt.show()
pm.summary(trace)
\end{pyverbatim}
\smallskip

Now that we have run the model we need to evaluate if the algorithm did a good job with
the estimation. We can get a sense of this by looking at diagnostic plots and metrics.
Lets first look at the visualizations of the MCMC-chain steps called traceplots:

\begin{figure}[H]
    \caption{Single Patient Traceplots}\label{traceplots}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{single_patient_traceplot.pdf}
\end{figure}

On the right in figure\ \ref{traceplots} we can see the timeline of the values of the
four parameters in the steps taken by each of our
chains (remember from chapter\ \ref{bayesinferencechallenges} that
each chain always evaluates all parameters). These are just the steps used for the
posterior estimation and don't include the
tuning steps. Each chain is portrayed with a different color. What we want to see
here is that the timelines looks like random noise around a single value with no obvious
patterns. If our distributions would be multimodal we would expect to see noise around
multiple values with maybe occasional noticeable jumps between these values. The one
thing we don't want to see is drift. If we would
see any patterns, like the chains moving values moving systematically up and down,
it would mean that the chains had problems moving trough the posterior and ended up
taking biased samples. These effects should be most pronounced at the beginning
steps and if we would see these, we should increase the number of tuning
steps\footnote{With this model it is hard to
demonstrate this problem effectively in PyMC3 as not having enough tuning and and
calculation steps just leads the model to fail outright without any results}.

% TODO try to do long chain without tuning and only plot the start

On the right side of the figure, we can see the distribution of the posterior values for
each chain. Here we
would want to see that distributions from different chains more or less align with
each other. If this is not the case, it means that the chains got stuck in different
parts of the posterior. This would also show on the left side plots with the different
chains staying in different parts of the posterior and being distinguishable from each
other. If we see this problem the solution is once again to increase the tuning steps.

Besides looking at plots, we can also estimate the success of the estimation from
diagnostic metrics. PyMC3 has an inbuilt method for printing the common metrics
(\pyv{pm.summary(trace)}). Below\ \ref{singlepatientdiagnosticmetrics} we can see the output
of this method for one of of the metrics (the full output is too big to display here):

\bigskip
\begin{figure}[H]
    \caption{Single Patient Traceplots}\label{singlepatientdiagnosticmetrics}
    \bigskip
    \centering
    \input{../figures/single_patient_diag_metrics.tex}
\end{figure}
\bigskip

The metric printed above is most important of the metrics \(\hat{R}\) (\pyv{r_hat}), that is short for potential scale
reduction statistic. This metrics tells us if
chains have converged to the equilibrium distribution. The metrics is calculated by
taking the variance of the drawn values for each chain,
then calculating the variance of drawn values for all chains combined and then taking the
ratio of average of the per chain variance compared to the pooled variance. If the
chains have converged to a common distribution the value of this metric will be one. If
the value is higher than one, this indicates that the chains have not converged. Small
divergences between the chains are not critical and \(\hat{R}\) values less than 1.1 should
not give cause for concern\ \cite{rhatrule}. In our case \(\hat{R}\) values are at or
very close to 1.0 for
all chains so everything seems to be in order.

If we find problems in the traceplots and diagnostic metrics and increasing the tuning
steps does not seem to help, we have to consider two things: the
MCMC estimation algorithm chosen by PyMC3 is not a good match for our model or our model might not
be properly defined. The first thing is try to manually choose a different algorithm
manually (the algorithm chosen by PyMC3 can be seen from the model computation printout
when running the model) and seeing
if we get better results. If this does not seem to work, we have to go over our model and
think if the model is in some way improperly defined. This could mean a simple mistake
in model definition or something more complex like the distributions that we chose for
our parameters not working well together or not fitting with the data.

One thing that we should not here is that we only looked at diagnostic values for our
parameters, but not the difference between the treatment means, that we are actually
most interested in. The reason for this, is that the difference is entirely determined
by the individual treatment parameters, so if the estimation of these is in order, so
will be the estimate of their difference.

\subsubsection*{Part 8: Checking the Posterior}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
pm.plot_posterior(trace)
\end{pyverbatim}
\smallskip

If the estimation looks fine, we can move on to checking the actual results and look at
the computed posterior distributions\ \ref{singlepatientposteriors}:

\bigskip
\begin{figure}[H]
    \caption{Single Patient Posterior Distributions}\label{singlepatientposteriors}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{single_patient_posteriors.pdf}
\end{figure}
\bigskip

The inbuilt PyMC3 method prints the posterior distribution for all of our parameters,
with their posterior mean and highest posterior density interval (HPD) for approximately
95 \% of the values. Here we see only 94 \% as the amount of steps does not make the
estimate granular enough and PyMC3 does every want to go over 95 \% even if it would
round to 95 \%. We can see that even though the posterior means between
the two treatment options are clearly different, there is quite a lot of overlap for the
HPD. Luckily we also defined the difference between these two parameters and can see
this as a separate plot and this gives us the result we are interested in. We can see
that the mean is positive and the HPD does not include zero, we can quite certain that
treatment A is a worse option for this patient than treatment B, with the likely values of
the difference falling between 0.71 and 0.94.

% TODO why is the difference not just the overlap between the individual parameter values?

\subsubsection*{Part 9: Checking Model Fit with Posterior Sampling}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
# posterior sampling
with single_patient_no_trend_model as model:
    post_pred = pm.sample_posterior_predictive(trace, samples=500)
    predictions = post_pred["y"]

draw_posterior_checks(
    predictions=predictions,
    measurements_df=measurements_df[patient_idx == 0],
    parameters_df=parameters_df[parameters_df["patient_idx"] == 0],
    plot_name="single_patient_posterior_sampling",
)
\end{pyverbatim}
\smallskip

Even though we already checked that the estimation seems to be working correctly and saw
that the posterior estimates clearly favor treatment option B for this patient, there is
still one important thing that we should check before announcing our results: we should
make sure that our model is actually a good fit for our data. Even if we have precise
results, if our model does a poor job at reflecting the
reality, we should not get too excited.

Checking model fit for Bayesian models can be done with posterior sampling where we generate samples from
the posterior distributions and for each of these samples we create a simulated dataset based
on the chosen parameter values that replicates the experiment. Then we compare these
simulated datasets to the actual
data. What we want to see is that actual data looks like a typical case for a simulated
dataset.

The logic behind this can be appreciated by thinking of a comparison between a case
where the real data is created with mean and a strong trend, but our model only includes the
mean. Even if our parameter estimates are very precise the data simulated based on our
model can never match the shape of the real data. On the other hand if we include the trend in our
model now the model replicates perfectly the data creation process and
the only difference with the real data and simulated data is that the real data was
created by the real parameter values, where as the simulated data was created with
estimated values for these parameters. Now the values of the of the simulated data
should spread around the real values with a purely random errors determined by the degree
of uncertainty we have on the posterior values. In the real world creating a model that
matches the data creation process exactly is rare outside laboratory experiments, but
the principle still holds: a good model should be a good description of the reality and
the data created based on its estimates should be hard to distinguish from the real
data, with little or no systematic errors.

% TODO what about if the our real data is weird and unrepresentative?

PyMC3 has an inbuilt method for taking the posterior samples, and we can use each of
these samples to recreate the time of measurements and see how well the simulated
timelines match with the real timeline. This can be seen from figure\ \ref{singlepatientposteriortimeline}.
All 500 samples are drawn as grey lines with one sample highlighted with a blue line and the actual measurement with a red line.
We can see that the results line up quite nicely and real data and the example sample
show very similar behavior.

\bigskip
\begin{figure}[H]
    \caption{Single Patient Posterior Sample Timeline}\label{singlepatientposteriortimeline}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{posterior_sample_timeline_single_patient.pdf}
\end{figure}
\bigskip

For more precise comparison we have written a special function
\pyv{draw_posteriors()}. We don't go over the code of this function,
but what it does is to use the datasets created by the PyMC3 inbuilt method and
calculates the mean of measurements under each treatment and difference between
these means from each posterior sample. It then plots the distribution of these means
against these values measures calculated from the
original data. For the difference between the means it also plots the actual
difference of the true parameters. Normally we would not know
this true value, but in this case we do as we generated the data ourselves and have
access to this parameters. This
makes it possible to see how accurate the model really is. The graphs created by this
function can be seen in figure\ \ref{singlepatientposteriorsampling}.

\bigskip
\begin{figure}[H]
    \caption{Single Patient Posterior Sampling}\label{singlepatientposteriorsampling}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{single_patient_posterior_sampling.pdf}
\end{figure}
\bigskip

% TODO legend should not overlay the graph

We see that the distribution of mean treatment effectiveness calculated from the
generated datasets matches nicely with the actual mean of the data. This means that our
model seems to be good enough approximation of reality. Also when we look at the difference in the treatment effectiveness,
we see that the posterior of difference between the treatments lines very nicely with the
true parameter difference, so our model also seem unbiased.

% TODO how to address accuracy?

\subsection{Final Results}

We have now completed the single patient analysis after going trough all the modelling steps:

\begin{enumerate}
    \item Defining the model
    \item Implementing the model in PyMC3
    \item Calculating the results
    \item Evaluating the estimation process
    \item Checking the posterior values
    \item Making sure that our model is an accurate enough description of reality
\end{enumerate}

In a real world analysis we would also add step 0: cleaning and exploring the data.
Based on our results we can say that for this patient treatment B is recommended over treatment A with
treatment B with the most probable difference being 0.82 measurement points in favor of treatment B. We can also easily communicate
the uncertainty in our estimation by with the HPD by saying that the true difference of
the treatments is with 95 \% probability between 0.71 and 0.94. As we have the full
posterior difference distribution we can also visually show these results with a graph,
which is the clinical setting probably the most advisable solution. The important thing
to underline here is that we are talking about actual probabilities and not likelihoods,
circumventing complicate definitions and having intuitively understandable results.

\section{Analyzing Multiple Trials With Hierarchical Models}\label{pooling}

Lets next add in all the other patients and connect the trials with a hierarchical
model. We will still focus on patient 1 that we analyzed previously and pay special
attention on how the posterior parameters estimates change when we pool all the data
that we have. Our suspicion is that estimate of the difference in treatment effectiveness
should be toned down as the treatment effectiveness for treatment B for patient 1 was an
outlier compared to the other patients (see figure\ \ref{parameterdistribution}).

\subsection{Defining the Model}\label{hiermodel}

We start with the same model that we used for the single patient, but now we index all our
measurements not just by time \(t\), but also by patient \(j\):

\begin{def}\label{hierarchicalmodel}
    \begin{equation}\label{}
        y_{tj} = Z_{tj}\theta_{j} + W_{tj}\eta_{j} + t\beta_{j} + \epsilon_{tj}
    \end{equation}
\end{def} where \(\theta_{j} \) and \(\eta_{j} \) are the treatment effects with \(Z_{tj}\) and \(W_{tj}\) being indicator
variables that is 1 when observation \(t\) is within a period where treatment was applied and
0 otherwise. \(\beta_j\) is the trend representing the natural progress of the
condition. \(\epsilon_{tj} \sim N(0,\tau_j^2) \) is the error term at \(t\) and
\(\rho_j\) is the correlation between consecutive errors.

To make this model hierarchical we now add the population level distributions which
the patient-level parameters are drawn from and the prior distributions for the
parameters defining the population level distribution. We add hierarchy for all the
parameters except patient the variance of the patient-level measurement error, as this
we are not really interested in the population-level of this parameter and adding it
would make the model unnecessarily complex.

\bigskip
\begin{tikzpicture}

    \matrix[matrix of math nodes, column sep=-75pt, row sep=30pt] (mat)
    {
        \ & \mu_{\mu_{\theta}}, \sigma_{\mu_{\theta}},
        \gamma_{\sigma_{\theta}},
        \mu_{\mu_{\eta}}, \sigma_{\mu_{\eta}},
        \gamma_{\sigma_{\eta}},
        \mu_{\mu_{\beta}}, \sigma_{\mu_{\beta}},
        \gamma_{\sigma_{\beta}} \\
        \ & \mu_{\theta}, \sigma_{\theta}, \mu_{\eta},\sigma_{\eta}, \mu_{\beta}, \sigma_{\beta}, \gamma_{\tau} & \ \\
        \ \\
        \ & \ & \gamma_{\tau} \\
        \ & \theta_j, \eta_j, \beta_j & \tau_j & \\\
        \ \\
        y_{1,1}, \ldots, y_{28, 1} & \ldots & y_{1,6}, \ldots, y_{28, 6} \\
    };

    \draw[->,>=latex] (mat-1-2) -- (mat-2-2);
    \draw[->,>=latex] (mat-2-2) -- (mat-5-2);
    \draw[->,>=latex] (mat-4-3) -- (mat-5-3);
    \draw[->,>=latex] (mat-5-2) -- (mat-7-1);
    \draw[->,>=latex] (mat-5-2) -- (mat-7-3);
    \draw[->,>=latex] (mat-5-3) -- (mat-7-1);
    \draw[->,>=latex] (mat-5-3) -- (mat-7-3);

    \node[anchor=east] at ([xshift = 0pt]mat-2-1)
    {$
        \mu_{\theta} \sim \mathcal{N}(\mu_{\mu_{\theta}}, \sigma_{\mu_{\theta}}),
        \sigma_{\theta} \sim HalfCauchy(0, \gamma_{\sigma_{\theta}}),
    $};
    \node[anchor=east] at ([xshift = 0pt]mat-3-1)
    {$
        \mu_{\eta} \sim \mathcal{N}(\mu_{\mu_{\eta}}, \sigma_{\mu_{\eta}}),
        \sigma_{\eta} \sim HalfCauchy(0, \gamma_{\sigma_{\eta}}),
    $};
    \node[anchor=east] at ([xshift = 0pt]mat-4-1)
    {$
        \mu_{\beta} \sim \mathcal{N}(\mu_{\mu_{\beta}}, \sigma_{\mu_{\beta}}),
        \sigma_{\beta} \sim HalfCauchy(0, \gamma_{\sigma_{\beta}})
    $};
    \node[anchor=east] at ([xshift = 0pt]mat-5-1)
    {$
        \theta_{j} \sim \mathcal{N}(\mu_{\theta},\sigma_{\theta}^2),
        \eta_{j} \sim \mathcal{N}(\mu_{\eta},\sigma_{\eta}^2),
        \beta_{j} \sim \mathcal{N}(\mu_{\beta},\sigma_{\beta}^2),
    $};
    \node[anchor=east] at ([xshift = 0pt]mat-6-1)
    {$
        \tau_{j} \sim HalfCauchy(0, \gamma_{\tau})
    $};
    \node[anchor=east] at ([xshift = -35pt]mat-7-1)
    {$
        y_{tj} = Z_{tj}\theta_{j} + W_{tj}\eta_{j} + t\beta_{j} + \epsilon_{tj}
    $};
\end{tikzpicture}

As the priors are defined on the population level we have to think about defining the
parameters of these distributions in a bit different way as we are now talking about
a different level of analysis. Once again we should turn to prior studies (once again
fictional). The best way to get the priors is if we can find a suitable meta-analyses that
pulls in relevant studies and list the average treatment effects observed. From this list
we could take the average and plug it into our population treatment effect mean and the
standard deviation of the values to the population treatment effect standard deviation.
Note that for standard deviation this is not mathematically entirely accurate (the means
observed from the studies are not true values from the population of the population mean, but and estimate
maximum likelihood estimate of it), but for our purposes this is fine as we just want a
rough estimate, that we can then tone down because of the uncertainty about the
applicability of the prior results. We pick mean values of 10 for both of the of
population treatment means and a standard deviation of 0.3. These are fairly informative
priors, but as we assume that these actually are based on prior studies this is okay.

For the standard deviances of the population-level treatment effect we can get an
estimate by looking at the distribution of the treatment effect standard deviations
across the studies.

% TODO Why variance is often defined as known in hierarchical models?

For the mean of the population level trend we stick with a mean of 0.1 and a standard
deviation of 0.01. This means that we think it is very certain that generally patients
get worse over time and this rate on average very close to 0.1. Where we have more
uncertainty is how much this trends varies within the population. We stick with fairly
non-informative 1 here.

The chosen priors values for our hierarchical
model are summarized in table\ \ref{hierarchicalmodelpriorvalues}.

\begin{table}[H]
    \caption{Hierarchical Model Prior Values}\label{hierarchicalmodelpriorvalues}
    \begin{align}\label{}
        \text{population treatment A mean mean prior: } & \mu_{\mu_{\theta}} = 10 \nonumber \\
        \text{population treatment A mean standard deviance prior: } & \sigma_{\mu_{\theta}} = 0.3 \nonumber \\
        \text{population treatment A standard deviance prior: } & \gamma_{\sigma_{\theta}} = 1 \nonumber \\
        \text{population treatment B mean mean prior: } & \mu_{\mu_{\eta}} = 10 \nonumber \\
        \text{population treatment B mean standard deviance prior: } & \sigma_{\mu_{\eta}} = 0.3 \nonumber \\
        \text{population treatment B standard deviance prior: } & \gamma_{\sigma_{\eta}} = 1 \nonumber \\
        \text{population trend mean mean prior: } & \mu_{\mu_{\beta}} = 0.1 \nonumber \\
        \text{population trend mean standard deviance prior: } & \sigma_{\mu_{\beta}} = 0.01 \nonumber \\
        \text{population trend standard deviance prior: } & \gamma_{\sigma_{\beta}} = 0.1 \nonumber \\
        \text{measurement error scale prior: } & \gamma_{\tau} = 1 \nonumber
    \end{align}
\end{table}

\subsection{Implementing the Model in PyMC3}

Next we move to implementing our hierarchical model in PyMC3. As by now we are already
familiar with the basics of PyMC3, we can now skip explaining the whole code and just focus
on the difference from the single patient model.

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]

with pm.Model() as hierarchical_model:

    # population priors
    pop_treatment_a_mean = pm.Normal(
        "Population Treatment A Mean", mu=10, sigma=1
    )
    pop_treatment_a_sd = pm.HalfCauchy("Population Treatment B Sd", beta=1)
    pop_treatment_b_mean = pm.Normal(
        "Population Treatment B Mean", mu=10, sigma=1
    )
    pop_treatment_b_sd = pm.HalfCauchy("Population Treatment B Sd", beta=1)
    pop_trend_mean = pm.Normal("Population Trend Mean", mu=0.1, sigma=0.3)
    pop_trend_sd = pm.HalfCauchy("Population Trend Sd", beta=2)

    # separate parameter for each patient
    pat_treatment_a = pm.Normal(
        "Treatment A",
        mu=pop_treatment_a_mean,
        sigma=pop_treatment_a_sd,
        shape=6,
    )
    pat_treatment_b = pm.Normal(
        "Treatment B",
        mu=pop_treatment_b_mean,
        sigma=pop_treatment_b_sd,
        shape=6,
    )
    pat_trend = pm.Normal(
        "trend",
        mu=pop_trend_mean,
        sigma=pop_trend_sd,
        shape=6,
    )
    # not hierarchical
    pat_gamma = pm.HalfCauchy(
        "Gamma", beta=1, shape=6,
    )

    measurement_means = (
        pat_treatment_a[patient_idx] * measurements_df["treatment_a"]
        + pat_treatment_b[patient_idx] * measurements_df["treatment_a"]
        + pat_trend[patient_idx] * measurements_df["measurement"]
    )

    likelihood = pm.Normal(
        "y",
        measurement_means,
        sigma=pat_gamma[patient_idx],
        observed=measurements_df["value"],
    )

    # adding the comparison between the treatments
    pop_difference = pm.Deterministic(
        "Population Treatment Difference (A-B)",
        pop_treatment_a_mean - pop_treatment_b_mean,
    )
    pat_difference = pm.Deterministic(
        "Treatment Difference (A-B)", pat_treatment_a - pat_treatment_b
    )

    trace = pm.sample(800, tune=300, cores=3)

    # checking traceplots separately for population and patients
    pm.traceplot(
        trace,
        [
            "Population Treatment A Mean",
            "Population Treatment A Sd",
            "Population Treatment B Mean",
            "Population Treatment B Sd",
            "Population Trend Mean",
            "Population Trend SD",
            "Population Gamma",
        ],
    )
    plt.show()
    pm.traceplot(
        trace, ["Treatment A", "Treatment B", "Trend", "Gamma"],
    )
    plt.show()
    pm.summary(trace)

    # plotting selected posteriors
    pm.plot_posterior(
        trace,
        [
            "Population Treatment A Mean",
            "Population Treatment B Mean",
            "Population Trend Mean",
            "Population Treatment Difference (A-B)",
        ],
    )
    plt.show()
    pm.plot_posterior(
        trace,
        [
            "Treatment Difference (A-B)",
        ],
    )
    plt.show()

# posterior sampling
with hierarchical_model as model:
    post_pred = pm.sample_posterior_predictive(trace, samples=500)
    predictions = post_pred["y"]

draw_posterior_checks(
    predictions=predictions,
    measurements_df=measurements_df,
    parameters_df=parameters_df,
    plot_name="hierarchical_model_posterior_sampling",
)
\end{pyverbatim}
\bigskip

\subsubsection*{Part 1: Defining the Population Level Priors}
\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
with pm.Model() as hierarchical_model:

    pop_treatment_a_mean = pm.Normal(
        "Population Treatment A Mean", mu=10, sigma=1
    )
    pop_treatment_a_sd = pm.HalfCauchy("Population Treatment B Sd", beta=1)
    pop_treatment_b_mean = pm.Normal(
        "Population Treatment B Mean", mu=10, sigma=1
    )
    pop_treatment_b_sd = pm.HalfCauchy("Population Treatment B Sd", beta=1)
    pop_trend_mean = pm.Normal("Population Trend Mean", mu=0.1, sigma=0.3)
    pop_trend_sd = pm.HalfCauchy("Population Trend Sd", beta=2)

\end{pyverbatim}
\bigskip

The first thing we do even before defining the population level priors is the create an
index variable of the patient index. This is done just for clarity as referring to
\pyv{patient_idx} instead of \pyv{measurements_df["patient_idx"]}.

With the population priors we use the exact same method that we used when we were
defining priors in the single patient model. All the distributions that we use are
inbuilt in PyMC3, so now complex procedures are needed here.

\subsubsection*{Part 2: Defining the Patient Level Parameters}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
    pat_treatment_a = pm.Normal(
        "Treatment A",
        mu=pop_treatment_a_mean,
        sigma=pop_treatment_a_sd,
        shape=6,
    )
    pat_treatment_b = pm.Normal(
        "Treatment B",
        mu=pop_treatment_b_mean,
        sigma=pop_treatment_b_sd,
        shape=6,
    )
    pat_trend = pm.Normal(
        "trend",
        mu=pop_trend_mean,
        sigma=pop_trend_sd,
        shape=6,
    )
    pat_gamma = pm.HalfCauchy(
        "Gamma", beta=1, shape=6,
    )
\end{pyverbatim}
\bigskip

When defining the patient level we run into bigger differences compared to the single
patient model. As every parameter has to be defined for each patient, every distribution
has to be multidimensional. As we saw when defining the single patient likelihood, PyMC3
can create multidimensional distributions if the distributions parameters are vectors
instead of single values, but now the parameters are single values (we have only one
population distribution), but we still want multidimensional distributions. We can do
this by explicitly defining the shape parameter of the distribution. This, a bit
confusingly named parameter (conflict with a commonly used terms for distribution
parameters), takes an integer value and makes the distribution have that many
dimensions, using the same parameter values for each dimensions. The parameter values
for the distributions we get from the population distributions.

\subsubsection*{Part 3: Defining the Likelihood}
\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
    measurement_means = (
        pat_treatment_a[patient_idx] * measurements_df["treatment_a"]
        + pat_treatment_b[patient_idx] * measurements_df["treatment_b"]
        + pat_trend[patient_idx] * measurements_df["measurement"]
    )

    likelihood = pm.Normal(
        "y",
        measurement_means,
        sigma=pat_gamma[patient_idx],
        observed=measurements_df["value"],
    )
\end{pyverbatim}
\bigskip

% TODO Make this clearer

Defining the likelihood is done in the same way as we did with the single patient model,
with imaging the data as one sample from a multidimensional distribution. In this case the
distribution will have 168 dimensions (we have 28 measurements from each of the 6 patients). We
create the means of this distribution by turning the treatment and trend means from
a vector of length 6 (one value for each patient) to a vector of length 168 with the
\pyv{patient_idx} vector. This vector has a length 168 and its values are the indices of the
patients with the first 28 values being
zero representing the measurements of patient 1, next 28 values being 1 representing
patient 2 and so on. By indexing the treatment mean and trend vectors with this vector the end
product is a vector of length 168, with each patient being represented 28 times
in a row. We then combine these vectors to a single treatment mean vector by multiplying
the treatment mean parts with the treatment indicators and the trend with the
measurement index (0-27) and then sum these together. The end result is a
vector of length 128 with each patient being represented with 28 sequential values that
combine the mean of treatment taken at that time point and the trend. The standard
deviation of the distribution is also turned into a similar type vector with the
\pyv{patient_idx} vector. Note that unlike in the single patient case where we could used a single
standard deviation value even though we had multiple means, in this case we have to
provide a vector with a length that matches the length of the vector of the means as we
want to use different standard deviations for the different patients.

\subsubsection*{Part 4: Running the Model}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
    trace = pm.sample(draw=1000, tune=700, cores=3)
\end{pyverbatim}
\bigskip

As our model is now much more complex than with the single patient model with much
more parameters, we need to use more tuning and calculation steps.
We bump the tuning steps from 500 to 700 and the calculation steps from 800 to 1000.
Otherwise nothing changes. We still use 3 chains and now these chains take
steps in the multidimensional posterior defined not just by patient parameters, but also
population parameters.

\subsubsection*{Part 5: Checking Algorithm Diagnostics}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
    pm.traceplot(
        trace,
        [
            "Population Treatment A Mean",
            "Population Treatment A Sd",
            "Population Treatment B Mean",
            "Population Treatment B Sd",
            "Population Trend Mean",
            "Population Trend SD",
        ],
    )
    plt.show()
    pm.traceplot(
        trace, ["Treatment A", "Treatment B", "Trend", "Gamma"],
    )
    plt.show()
    pm.summary(trace)
\end{pyverbatim}
\bigskip

When checking algorithm diagnostic the amount of parameters is now so big that it makes
sense to separate the traceplots into multiple chunks. We start with the population
traceplots as these look the same as with patient level traceplots in the single patient
model. In figure\ \ref{hierarchicalmodelpopulationtraceplots} we can see traceplots for each of
the 7 population level parameters. Once again, we hope to see that the timeseries looks
like noise and that the posterior distributions of the chains align nicely. Everything
seems to be in order.

\bigskip
\begin{figure}[H]
    \caption{Hierarchical Model Population Level Traceplots}\label{hierarchicalmodelpopulationtraceplots}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{hierarchical_model_population_level_traceplot.pdf}
\end{figure}
\bigskip

Next we check the traceplots of the patient level parameters (figure\
\ref{hierarchicalmodelpatienttraceplots}). These look bit different now as the patient
level parameters are multidimensional with every patient having their own
parameter. This make the right side of the plot hard to interpret, but left side is
still intelligible and see wee that the posteriors of the three chain align. It is
possible to plot the traceplots for single patients by passing the indices of parameters
to be plotted in a Python dictionary to ``coords'' parameter in the traceplot function.
This is more work, but is worth it if the right side of the plot would show some
problems. In this case everything looks okay, so we skip this.

\bigskip
\begin{figure}[H]
    \caption{Hierarchical Model Patient Level Traceplots}\label{hierarchicalmodelpatienttraceplots}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{hierarchical_model_patient_level_traceplot.pdf}
\end{figure}
\bigskip

Even if the graphs look good, it always a good idea to also check the values of the
\(\hat{R}\) (\pyv{r_hat}) diagnostic metric\ \ref{hierarchicalmodeldiagnosticmeasures}. As
the value for all parameters is 1.0 we can be confident that the chain have converged to
the equilibrium distribution.

\begin{figure}[H]
    \caption{Hierarchical Model Diagnostic Metrics}\label{hierarchicalmodeldiagnosticmeasures}
    \bigskip
    \centering
    \resizebox{!}{3.2in}{%
    \input{../figures/hierarchical_model_diag_metrics.tex}
    }
\end{figure}
\bigskip

\subsubsection*{Part 6: Checking the Posterior}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
    pm.plot_posterior(
        trace,
        [
            "Population Treatment A Mean",
            "Population Treatment B Mean",
            "Population Treatment Difference (A-B)",
            "Population Trend Mean",
        ],
    )
    plt.show()
    pm.plot_posterior(
        trace, ["Treatment Difference (A-B)"]
    )
    pm.plot_posterior(trace)
    plt.show()
\end{pyverbatim}
\bigskip

After checking the diagnostics, now its the time to see our results. As there are
now so many parameters we will only focus on the interesting ones. It is always of
course advisable to check every parameter to make sure that the results seem realistic,
but we skip this here for sake of brevity and clarity.

We start with the population level posterior that can be seen in figure\
\ref{hierarchicalmodelpopulationposteriors}. For both of the treatment effect means the
distribution means is exactly the same value that we used for simulating the data (10
for treatment A and 9.7 for treatment B). This gives us lots of confidence that our
model is working as intended. If we look at the distribution for treatment B we can see
that is has a thicker right tail. This is caused by the fact that our prior was set to
10, and then the data modified out estimate lower, leaving the tail bigger on the side
where our prior was. We compare this to treatment A, which has nearly identical
tails, and where the posterior mean is the same as the prior mean so the data caused no
shift. With the difference between the treatments, it is interesting to note that the
posterior mean of the population level treatment effects is not exactly the difference
between the posterior means 0.3, but 0.38. This is caused by the correlations between
the treatment mean posteriors. We also see that posterior 94 \% HPD does not include 0
so for this patient population it seems that we can quite certain that treatment B
generally works better. Note though that this does not mean that we can say that the it is highly
unlikely to see patients where treatment A is better, because we are just looking at the
distribution of mean and not the standard deviance. To understand how likely our model
would say it is to find a patient with better efficiency with treatment A, we would need
to create an average distribution based on the mean and standard deviation posteriors.

% TODO should I create the estimated population distribution?

\bigskip
\begin{figure}[H]
    \caption{Hierarchical Model Population-level Posterior Distributions}\label{hierarchicalmodelpopulationposteriors}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{hierarchical_model_population_level_posteriors.pdf}
\end{figure}
\bigskip

Next we turn to the patient level (figure\ \ref{hierarchicalmodelpatientposteriors}). As
there are so many parameter estimates to look on the patient side we just focus on
posterior estimates of the differences of the treatments as these numbers are our
primary reason for conducting the study. We can that for all patients except patient 3
the mean value is over 0 and also 0 is not included in the 95 \% HPD, meaning that for
all these patient treatment B can be reliably recommended. For patient 3 the mean is
negative that distribution is more on the negative side, but 0 is still included in the
95 \% HPD. For this patient we cannot be that sure about which treatment is better. We
should recommend treatment B, but we would have to underline that this recommendation is
not very certain. As a follow-up, we could recommend that this patient continues the
study to get more precise results. After the new measurements we can just repeat the
analysis without any major adjustments as with bayesian framework, this kind of
``peeking at the data'' does not break any statistical assumptions.

\bigskip
\begin{figure}[H]
    \caption{Hierarchical Model Patient-level Posterior Distributions}\label{hierarchicalmodelpatientposteriors}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{hierarchical_model_patient_level_posteriors.pdf}
\end{figure}
\bigskip

\subsubsection*{Part 7: Checking Model Fit with Posterior Sampling}

\bigskip
\begin{pyverbatim}[][fontsize=\footnotesize]
with hierarchical_model as model:
    post_pred = pm.sample_posterior_predictive(trace, samples=500)
    predictions = post_pred["y"]

draw_posterior_checks(
    predictions=predictions,
    measurements_df=measurements_df,
    parameters_df=parameters_df,
    plot_name="hierarchical_model_posterior_sampling",
)
\end{pyverbatim}
\bigskip

After seeing the results, it still necessary to do reality check with how well the
models match the actual data. We start by once again comparing the measurements timeline
of the posterior samples to the actual measurement timeline (figure\ \ref{hierarhicalmodelposteriortimeline})

\bigskip
\begin{figure}[H]
    \caption{Hierarchical Model Posterior Sample Timeline}\label{hierarhicalmodelposteriortimeline}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{posterior_sample_timeline_hierarchical_model.pdf}
\end{figure}
\bigskip

% TODO make a heuristic observation about how the original observations have to fit in
% the samples

For all patients the actual measurements lineup nicely with the samples, so our model
looks good. We also get a nice visual representation of how the size of the measurement errors of the
original measurements affects the posterior. For patient 6 the measurement errors are
very small and this the posterior estimates are tight and so is are samples created from
them.

To get more precision lets use our \pyv{draw_posterior_checks} function that we
used in single patient model. Even though the posterior predictions that we now get from
from the PyMC3 inbuilt \pyv{sample_posterior_predictive} function, our function is written
so that it can handle this new format and draw plots for each patient separately. The
resulting graphs can be seen in figure\ \ref{hierarchicalmodelposteriorsampling}.

\begin{figure}[H]
    \caption{Hierarchical Model Posterior Sample}\label{hierarchicalmodelposteriorsampling}
    \bigskip
    \centering
    \includegraphics[width=\textwidth,height=7.6in,keepaspectratio]{hierarchical_model_posterior_sampling.pdf}
\end{figure}

\subsection{Final Results}

We have now completed the hierarchical analysis. Based on the posterior estimates we can
recommend treatment A over treatment B for all patients except patient 3 where it is
uncertain which treatment is better as the posterior 95 \% HPD includes the value 0.

Based on the single patient analysis, we recommended treatment B for patient 1 and the
hierarchical analysis did not change that recommendation. But even if the recommendation
did not change, how did the posterior estimates differ? As this is such an important
question, lets compare the posterior estimates for treatment difference between the
single patient model and the hierarchical model for all patients (repeat the same
single patient model we fitted for patient 1 for all the patients).

In figure\ \ref{modelcomparisonpatient} we can see the posterior
treatment difference distribution for both model types. The differences are very slight
except for patient 2, where the results of the hierarchical model make the difference
between treatment noticeably smaller, and tightened the distribution. This looks a bit
underwhelming as it that, at least for out main outcome of interest, the hierarchical model did not make a
big difference, but this is not the whole story.

\bigskip
\begin{figure}[H]
    \caption{Patient 1 Treatment Effect Difference Non-hierarchical vs Hierarchical Model}\label{modelcomparisonpatient}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{model_comparison_patient.pdf}
\end{figure}
\bigskip

If instead looking at the estimated difference of the treatments, we look at estimates
for both of the treatments separately, we get a clearer picture of the effect of
hierarchical model. In figure\ \ref{posteriorshrinkage} we have plotted the estimated mean of
treatment efficacy for both of the treatment for both model types. From this graph it is
easy to see the effect of the hierarchical model: the estimates of the patients are drawn
towards each other. This is what ``shrinkage'' refers to. The reason why the changes are
hard to see from just looking at the treatment difference, is that the estimates of both
treatments tend to move toward the same direction with both parameter estimates either
increasing or decreasing.

\bigskip
\begin{figure}[H]
    \caption{Hierarchical Model Posterior Sample}\label{posteriorshrinkage}
    \bigskip
    \centering
    \includegraphics[width=4.5in,height=4.5in,keepaspectratio]{posterior_shrinkage.pdf}
\end{figure}
\bigskip

The interesting question here is what determines how much the
estimates change. It is clear that the model is not just drawing the estimates that are farthest from other observations
more towards each other, because we can see that estimates for patient 6 did not almost
move at all even though the single patient model estimates for this patient where quite
similar to patient 2, where we see a big move. The reason for this is that, yes, outlier
observations will get drawn more strongly towards others, but the other significant
factor determining the amount of this pull is how strong the estimates for single
patient are.

In figure\ \ref{modeltightness} we have plotted the posterior estimates distributions for the
treatment difference from the single-patient models. We can see that besides the
difference in means, there are also clear differences in how ``tight'' the estimates
are. Patients 2 and 6 are the outliers here, with patient 2 having a very wide
distribution and patient 6 having significantly tighter
distribution than other patient and n. This
is due to the fact that patient 6 had the smallest measurement error and patient 2 the
largest.

% TODO why does this have an effect?

\bigskip
\begin{figure}[H]
    \caption{Single Patient Model Posterior Estimate Distribution}\label{modeltightness}
    \bigskip
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{posterior_tightness.pdf}
\end{figure}
\bigskip

\chapter{Conclusion}


% TODO list most important points based in the chapters

What are N-of-1 designs and why we would want to use them

What are the challenges of modelling N-of-1 trials

What is bayesian inference and why it is so suitable to N-of-1 studies
% TODO is are model really safe for peaking?

How bayesian hierarchical models make it possible to intelligently pool data from
multiple studies

How is this pooling different from just a doing a group level analysis

Implementing N-of-1 bayesian modelling with Python and PyMC3

% TODO what did we not cover

Data exploration

Model model comparison. With real data we should test multiple model starting from the
simplest possible.

Where to find information about model comparison?

\addcontentsline{toc}{chapter}{Bibliography}

\begin{thebibliography}{4}

    \bibitem{nofone}
    Richard L. Kravitz, Naihua Duan, Sunita Vohra, Jiang Li: Introduction to to
    N-of-1 Trials: Indications and Barriers in Design and Implementation of
    N-of-1 Trials: A User's Guide, AHRQ, 2014.

    \bibitem{costs}
    Wilson D. Pace, Elizabeth W. Staton, Eric B. Larson: Financing and Economics
    of Conducting N-of-1 Trials in Design and Implementation of N-of-1 Trials: A
    User's Guide, AHRQ, 2014.

    \bibitem{stat}
    Christopher H. Schmid, Naihua Duan: Statistical Design and Analytic
    Considerations in Design and Implementation of N-of-1 Trials: A User's
    Guide, AHRQ, 2014.

    \bibitem{clinbayes}
    Christoper J. Gill, Lora Savin, Christopher H. Schmid: Why clinicians are
    natural Bayesians, British Medical Journal 2005;330(7499):1080-1083.

    \bibitem{diseaseburden}
    Colin D. Mathers, Dejan Loncar: Projections of global mortality and burden
    of disease from 2002 to 2030, PLoS medicine 2016;3.11:e442.

    \bibitem{cgm}
    Irl B. Hirsch, et al.: Clinical application of emerging sensor technologies
    in diabetes management: consensus guidelines for continuous glucose
    monitoring (CGM). Diabetes Technology and Therapeutics 2018;10.4:232-246.

    \bibitem{kruschke}
    John K. Kruschke: Doing Bayesian Data Analysis: A Tutorial With R, JAGS, and Stan
    (2nd), Academic Press, 2014.

    \bibitem{gelman}
    Andrew Gelman, John B. Carlin, Hal Stern, David Dunson, Aki Vehtari, Donald B.
    Rubin: Bayesian Data Analysis, CRC Press, 2013

    \bibitem{HTE1}
    Peter Rothwell: External Validity of Randomised Controlled Trials: To Whom Do the
    Results of This Trial Apply?, Lancet, Jan 1-7 2005;365(9453):82-93.15

    \bibitem{HTE2}
    Peter Rothwell: Treating individuals 2. Subgroup Analysis in Randomised Controlled
    Trials: Importance, Indications, and Interpretation, Lancet, Jan 8-14
    2005;365(9454):176-186.16

    \bibitem{HTE3}
    Diane Warden, John Rush, Madhukar Trivedi, et al.: The STAR*D Project Results: A Comprehensive Review of Findings, Current Psychiatry Reports, Dec 2007;9(6):449-459

    \bibitem{HTE4}
    David Kent, Peter Rothwell, John Ioannidis, et al.: Assessing and Reporting Heterogeneity in
    Treatment Effects in Clinical Trials: A Proposal, Trials, 2010;11:85

    \bibitem{pymc3}
    John Salvatier, Thomas Wiecki, Christopher Fonnesbeck: Probabilistic Programming in Python Using PyMC3, PeerJ Computer
    Science, 2016, 2:e55

    \bibitem{variancepriors}
    Andrew Gelman: Prior distributions for variance parameters in hierarchical models
    (comment on article by Browne and Draper). Bayesian analysis 1.3, 2006;515-534.

    \bibitem{rhatrule}
    Jonah Gabry, Ben Goodrich:
    https://cran.r-project.org/web/packages/rstanarm/vignettes/rstanarm.html, 2020.
    
    \bibitem{nuts}
    Matthew Hoffman, Andrew Gelman: The No-U-Turn sampler: adaptively setting path
    lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research. 15.1, 2014;1593-1623.

    \bibitem{github}
    Tuomo Kareoja: https://github.com/TuomoKareoja/hierarchical-bayes-nof1-thesis,
    GitHub repository, 2020.

\end{thebibliography}
\end{document}
